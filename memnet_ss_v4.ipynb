{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"memnet_ss_v4.ipynb","provenance":[],"collapsed_sections":["KuMYUztgp6h_"],"machine_shape":"hm","authorship_tag":"ABX9TyPOUa4FYjJXA/+acACA7w30"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DS9_H89C0o1b"},"source":["## Module setup"]},{"cell_type":"markdown","metadata":{"id":"4a0uXkmyFWMe"},"source":["# Changes: \n","1) Training set: BSDS200, T91, General100\n","\n","2) training patch 31x31 and stride is 21."]},{"cell_type":"code","metadata":{"id":"khmWyAd17fbU"},"source":["import os\n","import tarfile\n","import glob\n","import io\n","import random\n","from tqdm import tqdm\n","import shutil\n","import tarfile\n","import PIL\n","from IPython.display import display, Image\n","import numpy as np\n","from six.moves.urllib.request import urlretrieve\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision import transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KuMYUztgp6h_"},"source":["## Helper \n","# RGB -> YCbCr and YCbCr -> RGB image conversion\n","This part is taken from Kornia\n","https://kornia.readthedocs.io/en/latest/_modules/kornia/color/ycbcr.html"]},{"cell_type":"code","metadata":{"id":"ZJlq-WlfqQ69"},"source":["def rgb_to_ycbcr(image: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Convert an RGB image to YCbCr.\n","\n","    Args:\n","        image (torch.Tensor): RGB Image to be converted to YCbCr with shape :math:`(*, 3, H, W)`.\n","\n","    Returns:\n","        torch.Tensor: YCbCr version of the image with shape :math:`(*, 3, H, W)`.\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> output = rgb_to_ycbcr(input)  # 2x3x4x5\n","    \"\"\"\n","    if not isinstance(image, torch.Tensor):\n","        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n","            type(image)))\n","\n","    if len(image.shape) < 3 or image.shape[-3] != 3:\n","        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n","                         .format(image.shape))\n","\n","    r: torch.Tensor = image[..., 0, :, :]\n","    g: torch.Tensor = image[..., 1, :, :]\n","    b: torch.Tensor = image[..., 2, :, :]\n","\n","    delta: float = 0.5\n","    y: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n","    cb: torch.Tensor = (b - y) * 0.564 + delta\n","    cr: torch.Tensor = (r - y) * 0.713 + delta\n","    return torch.stack([y, cb, cr], -3)\n","\n","\n","\n","def ycbcr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Convert an YCbCr image to RGB.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Args:\n","        image (torch.Tensor): YCbCr Image to be converted to RGB with shape :math:`(*, 3, H, W)`.\n","\n","    Returns:\n","        torch.Tensor: RGB version of the image with shape :math:`(*, 3, H, W)`.\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> output = ycbcr_to_rgb(input)  # 2x3x4x5\n","    \"\"\"\n","    if not isinstance(image, torch.Tensor):\n","        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n","            type(image)))\n","\n","    if len(image.shape) < 3 or image.shape[-3] != 3:\n","        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n","                         .format(image.shape))\n","\n","    y: torch.Tensor = image[..., 0, :, :]\n","    cb: torch.Tensor = image[..., 1, :, :]\n","    cr: torch.Tensor = image[..., 2, :, :]\n","\n","    delta: float = 0.5\n","    cb_shifted: torch.Tensor = cb - delta\n","    cr_shifted: torch.Tensor = cr - delta\n","\n","    r: torch.Tensor = y + 1.403 * cr_shifted\n","    g: torch.Tensor = y - 0.714 * cr_shifted - 0.344 * cb_shifted\n","    b: torch.Tensor = y + 1.773 * cb_shifted\n","    return torch.stack([r, g, b], -3)\n","\n","\n","\n","class RgbToYcbcr(nn.Module):\n","    \"\"\"Convert an image from RGB to YCbCr.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Returns:\n","        torch.Tensor: YCbCr version of the image.\n","\n","    Shape:\n","        - image: :math:`(*, 3, H, W)`\n","        - output: :math:`(*, 3, H, W)`\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> ycbcr = RgbToYcbcr()\n","        >>> output = ycbcr(input)  # 2x3x4x5\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(RgbToYcbcr, self).__init__()\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        return rgb_to_ycbcr(image)\n","\n","\n","\n","class YcbcrToRgb(nn.Module):\n","    \"\"\"Convert an image from YCbCr to Rgb.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Returns:\n","        torch.Tensor: RGB version of the image.\n","\n","    Shape:\n","        - image: :math:`(*, 3, H, W)`\n","        - output: :math:`(*, 3, H, W)`\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> rgb = YcbcrToRgb()\n","        >>> output = rgb(input)  # 2x3x4x5\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(YcbcrToRgb, self).__init__()\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        return ycbcr_to_rgb(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eO181ezuqXK6"},"source":["## Directory setup and preparing the datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6cRBsi_0Z2s","executionInfo":{"status":"ok","timestamp":1618844374380,"user_tz":240,"elapsed":24839,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"efa73901-b27c-472f-d610-284030dbf808"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z9bnrCCa4Y_E"},"source":["directory = '/content/memnet'\n","if not os.path.exists(directory):\n","  os.makedirs(directory)\n","\n","os.chdir(directory)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcghdQBZzYhe"},"source":["# !ls /content/drive/MyDrive/memnet\n","# !cp /content/drive/MyDrive/memnet/t91_crop_training.tar.gz /content/memnet/t91_crop_training.tar.gz\n","# with tarfile.open('t91_crop_training.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !cp /content/drive/MyDrive/memnet/bsds_crop_training.tar.gz /content/memnet/bsds_crop_training.tar.gz\n","# with tarfile.open('bsds_crop_training.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !ls /content/memnet/t91_crop_training/ -1 | wc -l\n","# !ls /content/memnet/bsds_crop_training/ -1 | wc -l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uO1JLjItq4Pd"},"source":["## Model : Single-Supervised MemNet "]},{"cell_type":"code","metadata":{"id":"A5rUr_6uDYZg"},"source":["class FeatExtBlock(nn.Module):\n","    def __init__(self, in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1):\n","        super(FeatExtBlock, self).__init__()\n","        self.feature = nn.Sequential(nn.BatchNorm2d(1), nn.ReLU(),\n","                                     nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n","                                               kernel_size=kernel_size, stride=stride,\n","                                               padding=padding, bias=False))\n","\n","    def forward(self, x):\n","        fe_out = self.feature(x)\n","        return fe_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoywv9Bk5HNp"},"source":["class ResidualBlock(nn.Module):\n","  def __init__(self, n_layers=2, n_channels=64, kernel_size=3, stride=1, padding=1):\n","    super(ResidualBlock, self).__init__()\n","    layers = []\n","    for _ in range(n_layers):\n","      layers.append(nn.BatchNorm2d(n_channels))\n","      layers.append(nn.ReLU())\n","      layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n","                              stride=stride, padding=padding, bias=False))\n","    \n","    self.residual = nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    out = self.residual(x)\n","    return out + x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awmHxvKb69Xl"},"source":["class RecursiveUnit(nn.Module):\n","  def __init__(self, n_short=6, d_conv=64):\n","    super(RecursiveUnit, self).__init__()\n","    self.n_short = n_short\n","    self.residual = nn.ModuleList([ResidualBlock(n_layers=2, n_channels=d_conv) for _ in range(n_short)])\n","\n","  def forward(self, b_prev_m):\n","    Hs = []\n","    Hs.append(self.residual[0](b_prev_m))\n","    for rec in range(1, self.n_short):\n","      Hs.append(self.residual[rec](Hs[-1]))\n","    b_short = torch.cat(Hs, dim=1)\n","    return b_short"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CqUd6dgpRBgA"},"source":["class GateUnit(nn.Module):\n","    def __init__(self, in_channels, d_conv=64):\n","        super(GateUnit, self).__init__()\n","        self.in_channels = in_channels\n","        self.gate_layer = nn.Sequential(nn.BatchNorm2d(in_channels), nn.ReLU(),\n","                                  nn.Conv2d(in_channels=self.in_channels, out_channels=d_conv, kernel_size=1,\n","                                            stride=1, padding=0, bias=False))\n","\n","    def forward(self, b_gate):\n","        b_mem = self.gate_layer(b_gate)\n","        return b_mem"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8MftIwz-CIk"},"source":["class MemBlock(nn.Module):\n","  def __init__(self, n_long, N_short=6, d_conv=64):\n","    super(MemBlock, self).__init__()\n","    self.n_long = n_long\n","    self.N_long = N_short\n","    self.d_conv = d_conv\n","    self.gate_in = (n_long + N_short) * d_conv\n","    self.short_unit = RecursiveUnit(N_short, d_conv)\n","    self.gate = GateUnit(self.gate_in, self.d_conv)\n","\n","  def forward(self, b_long):\n","    b_short = self.short_unit(b_long[:, -self.d_conv:, :, :])\n","    b_gate = torch.cat([b_short, b_long], dim=1)\n","    b_m = self.gate(b_gate)\n","    return b_m"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZVQ3ZRZFLNj"},"source":["class ReconstBlock(nn.Module):\n","  def __init__(self, d_conv=64, kernel_size=3, stride=1, padding=1):\n","    super(ReconstBlock, self).__init__()\n","    self.d_conv = d_conv\n","    self.recon = nn.Sequential(nn.BatchNorm2d(d_conv), nn.ReLU(),\n","                               nn.Conv2d(in_channels=d_conv, out_channels=1, kernel_size=kernel_size,\n","                                         stride=stride, padding=padding, bias=False))\n","\n","  def forward(self, b_m, x):\n","    y = self.recon(b_m)\n","    return y+x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEyTxqG0xfAK"},"source":["class MemNet_SS(nn.Module):\n","    def __init__(self, N_long, N_short, d_conv=64):\n","        super(MemNet_SS, self).__init__()\n","        self.N_long = N_long\n","        self.N_short = N_short\n","        self.d_conv = d_conv\n","        self.fe_block = FeatExtBlock(1, d_conv)\n","        self.mem_blocks = nn.ModuleList([MemBlock(n_mem+1, N_short, d_conv) for n_mem in range(N_long)])\n","        self.recon_block = ReconstBlock(d_conv)\n","        # This part is taken from: https://discuss.pytorch.org/t/contraining-weights-to-sum-to-1/20609/2\n","        # self.eps = 1E-7\n","        # self.linavg = nn.Parameter((1./self.N_long) * torch.ones(N_long), requires_grad=True)\n","\n","    def forward(self, x):\n","      fe = self.fe_block(x)\n","      b_ms = []\n","      b_ms.append(fe)\n","      y = torch.zeros_like(x)\n","      for n_mem in range(self.N_long):\n","        b_long = torch.cat(b_ms, dim=1)\n","        b_ms.append(self.mem_blocks[n_mem](b_long))\n","        # self.linavg = self.linavg.clamp(min=self.eps) # in case weights > 0\n","        # linavg_sum = self.linavg.sum(0, keepdim=True)\n","        # y += self.linavg[n_mem] * self.recon_block(b_ms[-1], x) / linavg_sum\n","      \n","      y = self.recon_block(b_ms[-1], x)\n","      return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sszPhXlL1B6b","executionInfo":{"status":"ok","timestamp":1618844380706,"user_tz":240,"elapsed":1510,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"23f3304d-0500-403a-ce2b-35a198d50d9f"},"source":["def size_check():\n","  p = torch.rand(2, 1, 4, 4)\n","  print(p.size())\n","  d_conv = 8\n","  fe_model = FeatExtBlock(1, d_conv)\n","  q = fe_model(p)\n","  print(q.size())\n","  mem_model = MemBlock(1, 6, d_conv)\n","  r = mem_model(q)\n","  print(r.size())\n","  recon_model = ReconstBlock(d_conv)\n","  s = recon_model(r, p)\n","  print(s.size())\n","  model = MemNet_SS(2, 2, 32)\n","  t = model(p)\n","  print(t.size())\n","\n","size_check()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([2, 1, 4, 4])\n","torch.Size([2, 8, 4, 4])\n","torch.Size([2, 8, 4, 4])\n","torch.Size([2, 1, 4, 4])\n","torch.Size([2, 1, 4, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zkrs29MLquvQ"},"source":["## Dataset Preparation"]},{"cell_type":"code","metadata":{"id":"6R5hB9SzAzpi"},"source":["class DatasetSR(Dataset):\n","  def __init__(self, images_dir, transform=None):\n","    self.files = []\n","    for dir in images_dir:\n","      self.files += glob.glob(dir + '/*')\n","    self.files = sorted(self.files)\n","    self.transform=transform\n","  \n","  def __len__(self):\n","    return len(self.files)\n","\n","  def __getitem__(self, index):\n","    image_path = self.files[index]\n","    hr_image = PIL.Image.open(image_path)\n","    hr_image = hr_image.convert('YCbCr')\n","    # random_gen = int(image_path.split('/')[-1].split('_')[-2][-1]) + int(image_path.split('/')[-1].split('_')[-1][-5])\n","    # scale = random_gen % 3 + 2 \n","    scale = random.randint(2, 4)\n","    width_down = hr_image.width // scale\n","    heigth_down = hr_image.height // scale\n","    lr_image = hr_image.resize((width_down, heigth_down),\n","                               resample=PIL.Image.BICUBIC)\n","    lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                               resample=PIL.Image.BICUBIC)\n","    \n","    trans_toten = transforms.ToTensor()\n","    hr_ten = trans_toten(hr_image)\n","    lr_ten = trans_toten(lr_image)\n","    hr_ten = rgb_to_ycbcr(hr_ten)\n","    lr_ten = rgb_to_ycbcr(lr_ten)\n","    \n","    if self.transform:\n","            hr = self.transform(hr_image)\n","            lr = self.transform(lr_image)\n","\n","    return hr, lr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_mQTJeIrHfl"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"2r1QT0atLrnY"},"source":["class AttrDict(dict):\n","  def __init__(self, *args, **kwargs):\n","    super(AttrDict, self).__init__(*args, **kwargs)\n","    self.__dict__ = self\n","\n","def weights_init(m):\n","  if isinstance(m, nn.Conv2d):\n","    nn.init.xavier_normal_(m.weight.data, gain=1.0)\n","    # nn.init.zeros_(m.bias.data)\n","\n","# The following function si taken from: https://medium.com/analytics-vidhya/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61\n","def load_ckp(checkpoint_fpath, model):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    model.load_state_dict(checkpoint)\n","    # optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model\n","\n","def train(args):\n","  model = MemNet_SS(args.N_long, args.N_short, args.d_conv)\n","  model.train()\n","  if torch.cuda.is_available():\n","    print(\"sending model to cuda...\")\n","    model = model.cuda()\n","  # optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","  optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.w_decay)\n","  if args['start_from']:\n","    print('starting from {}'.format(args.start_from))\n","    checkpoint = torch.load(args.start_from)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    # model = load_ckp(args['start_from'], model)\n","  else:\n","    print(\"initializing the model...\")\n","    model.apply(weights_init)\n","  \n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.epochs//8, gamma=args.gamma)\n","  criterion = nn.MSELoss(reduction='sum')\n","  trnsfrm = transforms.Compose([transforms.ToTensor()])\n","  # dataset = Dataset(opt.images_dir, opt.patch_size, opt.jpeg_quality, opt.use_fast_loader)\n","  dataset = DatasetSR(images_dir=args.train_dir, transform=trnsfrm)\n","  dataloader = DataLoader(dataset=dataset,\n","                          batch_size=args.batch_size,\n","                          shuffle=True,\n","                          num_workers=args.threads,\n","                          pin_memory=True,\n","                          drop_last=False)\n","  \n","  print('Training starts...')\n","  start = time.time()\n","  train_losses = []\n","  valid_losses = []\n","  for epoch in range(args.epochs):\n","    losses = []\n","    for i, data in enumerate(dataloader):\n","      y, x = data\n","      x = x[:, 0, :, :].unsqueeze(1)\n","      y = y[:, 0, :, :].unsqueeze(1)\n","      if torch.cuda.is_available():\n","        x = x.cuda()\n","        y = y.cuda()\n","      \n","      x_hat = model(x)\n","      # print(\"evaluated\")\n","      loss = (0.5/args.batch_size)*criterion(x_hat, y)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      nn.utils.clip_grad_norm_(model.parameters(), args.clip) \n","      optimizer.step()\n","      losses.append(loss.data.item())\n","\n","    # print(\"moving forward...\")\n","    scheduler.step()\n","    avg_loss = np.mean(losses)\n","    train_losses.append(avg_loss)\n","    time_elapsed = time.time() - start\n","    print('Epoch [%d/%d], Loss: %.4f, Time (s): %d'\n","          % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n","        )\n","    if (epoch+1) % 5 == 0:\n","      patch_checkpoint = os.path.join(args.output_dir, 'memnet_epoch_{}.pt'.format(epoch+1))\n","      torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, patch_checkpoint)\n","      # torch.save(model.state_dict(), os.path.join(args.output_dir, 'memnet_epoch_{}.pt'.format(epoch+1)))\n","\n","  # Plot training curve\n","  plt.figure()\n","  plt.plot(train_losses, \"ro-\", label=\"Train\")\n","  # plt.plot(valid_losses, \"go-\", label=\"Validation\")\n","  plt.legend()\n","  plt.title(\"Loss\")\n","  plt.xlabel(\"Epochs\")\n","  plt.show()\n","  plt.savefig(os.path.join(args.output_dir, \"train_loss.png\"))\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfRGHdI1PPQu"},"source":["drive_dir = '/content/drive/MyDrive/memnet/memnet_ss_v3'\n","if not os.path.exists(drive_dir):\n","  os.makedirs(drive_dir)\n","\n","patch_size = 31\n","trainings = ['BSDS200_p'+str(patch_size), 'T91_p'+str(patch_size), 'General100_p'+str(patch_size)]\n","args = AttrDict()\n","args_dict = {\n","    'threads' : 1,\n","    'kernel_size' : 3,\n","    'N_long' : 6,\n","    'N_short' : 6,\n","    'd_conv' : 64,\n","    'lr' : 0.1,\n","    'momentum' : 0.9,\n","    'w_decay' : 1E-4,\n","    'clip' : 0.4,\n","    'gamma' : 0.2,\n","    'batch_size':128,\n","    'patch_size': patch_size,\n","    'epochs' : 80,\n","    'train_dir' : [directory+'/'+trainings[i] for i in range(len(trainings))],\n","    'output_dir' : drive_dir,\n","    'start_from' : ''\n","}\n","args.update(args_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCGLKe5dTbNk","executionInfo":{"status":"ok","timestamp":1618844464696,"user_tz":240,"elapsed":28014,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"51cb1e9c-c1ff-4600-ac8b-90d72ea7e73a"},"source":["gdrive_tdir = '/content/drive/MyDrive/memnet'\n","for tri in trainings:\n","  shutil.copy(os.path.join(gdrive_tdir, tri + '.tar.gz'), directory)\n","  file = tarfile.open(os.path.join(directory, tri + '.tar.gz'))\n","  file.extractall('./')\n","  file.close()\n","  print(len(next(os.walk('./'+tri))[2]))\n","\n","# !ls /content/drive/MyDrive/memnet\n","# !cp /content/drive/MyDrive/memnet/T91_p51.tar.gz /content/memnet/T91_p51.tar.gz\n","# with tarfile.open('T91_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !cp /content/drive/MyDrive/memnet/BSDS200_p51.tar.gz /content/memnet/BSDS200_p51.tar.gz\n","# with tarfile.open('BSDS200_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !cp /content/drive/MyDrive/memnet/General100_p51.tar.gz /content/memnet/General100_p51.tar.gz\n","# with tarfile.open('General100_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !ls /content/memnet/T91_p51/ -1 | wc -l\n","# !ls /content/memnet/BSDS200_p51/ -1 | wc -l\n","# !ls /content/memnet/General100_p31/ -1 | wc -l"],"execution_count":null,"outputs":[{"output_type":"stream","text":["73600\n","13115\n","42803\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uoGJUelbTfGc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1618886762326,"user_tz":240,"elapsed":42294881,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"59dbcbd5-88e2-499d-b5b1-f5b5200d9aea"},"source":["memnet = train(args)\n","while True:pass"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sending model to cuda...\n","initializing the model...\n","Training starts...\n","Epoch [1/80], Loss: 6.7078, Time (s): 515\n","Epoch [2/80], Loss: 1.1553, Time (s): 1030\n","Epoch [3/80], Loss: 1.1186, Time (s): 1546\n","Epoch [4/80], Loss: 1.1005, Time (s): 2061\n","Epoch [5/80], Loss: 1.0905, Time (s): 2576\n","Epoch [6/80], Loss: 1.0809, Time (s): 3092\n","Epoch [7/80], Loss: 1.0808, Time (s): 3607\n","Epoch [8/80], Loss: 1.0746, Time (s): 4122\n","Epoch [9/80], Loss: 1.0754, Time (s): 4637\n","Epoch [10/80], Loss: 1.0757, Time (s): 5153\n","Epoch [11/80], Loss: 1.0332, Time (s): 5668\n","Epoch [12/80], Loss: 1.0325, Time (s): 6183\n","Epoch [13/80], Loss: 1.0284, Time (s): 6698\n","Epoch [14/80], Loss: 1.0289, Time (s): 7214\n","Epoch [15/80], Loss: 1.0202, Time (s): 7729\n","Epoch [16/80], Loss: 1.0235, Time (s): 8244\n","Epoch [17/80], Loss: 1.0176, Time (s): 8759\n","Epoch [18/80], Loss: 1.0167, Time (s): 9275\n","Epoch [19/80], Loss: 1.0163, Time (s): 9790\n","Epoch [20/80], Loss: 1.0161, Time (s): 10305\n","Epoch [21/80], Loss: 1.0050, Time (s): 10821\n","Epoch [22/80], Loss: 0.9986, Time (s): 11336\n","Epoch [23/80], Loss: 0.9979, Time (s): 11851\n","Epoch [24/80], Loss: 0.9950, Time (s): 12366\n","Epoch [25/80], Loss: 0.9962, Time (s): 12881\n","Epoch [26/80], Loss: 0.9926, Time (s): 13396\n","Epoch [27/80], Loss: 0.9901, Time (s): 13912\n","Epoch [28/80], Loss: 0.9916, Time (s): 14428\n","Epoch [29/80], Loss: 0.9899, Time (s): 14943\n","Epoch [30/80], Loss: 0.9915, Time (s): 15459\n","Epoch [31/80], Loss: 0.9852, Time (s): 15974\n","Epoch [32/80], Loss: 0.9836, Time (s): 16490\n","Epoch [33/80], Loss: 0.9841, Time (s): 17005\n","Epoch [34/80], Loss: 0.9806, Time (s): 17520\n","Epoch [35/80], Loss: 0.9813, Time (s): 18036\n","Epoch [36/80], Loss: 0.9854, Time (s): 18552\n","Epoch [37/80], Loss: 0.9780, Time (s): 19067\n","Epoch [38/80], Loss: 0.9794, Time (s): 19583\n","Epoch [39/80], Loss: 0.9785, Time (s): 20098\n","Epoch [40/80], Loss: 0.9849, Time (s): 20614\n","Epoch [41/80], Loss: 0.9767, Time (s): 21130\n","Epoch [42/80], Loss: 0.9830, Time (s): 21645\n","Epoch [43/80], Loss: 0.9809, Time (s): 22161\n","Epoch [44/80], Loss: 0.9791, Time (s): 22676\n","Epoch [45/80], Loss: 0.9833, Time (s): 23192\n","Epoch [46/80], Loss: 0.9803, Time (s): 23707\n","Epoch [47/80], Loss: 0.9805, Time (s): 24223\n","Epoch [48/80], Loss: 0.9781, Time (s): 24738\n","Epoch [49/80], Loss: 0.9791, Time (s): 25254\n","Epoch [50/80], Loss: 0.9816, Time (s): 25769\n","Epoch [51/80], Loss: 0.9772, Time (s): 26284\n","Epoch [52/80], Loss: 0.9812, Time (s): 26800\n","Epoch [53/80], Loss: 0.9782, Time (s): 27315\n","Epoch [54/80], Loss: 0.9760, Time (s): 27830\n","Epoch [55/80], Loss: 0.9810, Time (s): 28345\n","Epoch [56/80], Loss: 0.9769, Time (s): 28861\n","Epoch [57/80], Loss: 0.9787, Time (s): 29376\n","Epoch [58/80], Loss: 0.9815, Time (s): 29892\n","Epoch [59/80], Loss: 0.9790, Time (s): 30407\n","Epoch [60/80], Loss: 0.9775, Time (s): 30922\n","Epoch [61/80], Loss: 0.9781, Time (s): 31438\n","Epoch [62/80], Loss: 0.9808, Time (s): 31953\n","Epoch [63/80], Loss: 0.9810, Time (s): 32469\n","Epoch [64/80], Loss: 0.9746, Time (s): 32984\n","Epoch [65/80], Loss: 0.9801, Time (s): 33500\n","Epoch [66/80], Loss: 0.9807, Time (s): 34015\n","Epoch [67/80], Loss: 0.9784, Time (s): 34530\n","Epoch [68/80], Loss: 0.9829, Time (s): 35045\n","Epoch [69/80], Loss: 0.9792, Time (s): 35561\n","Epoch [70/80], Loss: 0.9808, Time (s): 36076\n","Epoch [71/80], Loss: 0.9800, Time (s): 36591\n","Epoch [72/80], Loss: 0.9804, Time (s): 37106\n","Epoch [73/80], Loss: 0.9753, Time (s): 37622\n","Epoch [74/80], Loss: 0.9787, Time (s): 38137\n","Epoch [75/80], Loss: 0.9808, Time (s): 38652\n","Epoch [76/80], Loss: 0.9821, Time (s): 39167\n","Epoch [77/80], Loss: 0.9813, Time (s): 39682\n","Epoch [78/80], Loss: 0.9786, Time (s): 40197\n","Epoch [79/80], Loss: 0.9804, Time (s): 40713\n","Epoch [80/80], Loss: 0.9818, Time (s): 41228\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-b289ffe118e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmemnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]}]}