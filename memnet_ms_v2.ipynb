{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"memnet_ms_v2.ipynb","provenance":[],"collapsed_sections":["KuMYUztgp6h_"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyM3kg1GE3g1Wm2YhI+dfqwM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DS9_H89C0o1b"},"source":["## Module setup"]},{"cell_type":"markdown","metadata":{"id":"4a0uXkmyFWMe"},"source":["# Changes: \n","1) Training set: BSDS200, T91, General100.\n","\n","2) training patch 51x51 and stride is 41."]},{"cell_type":"code","metadata":{"id":"khmWyAd17fbU","executionInfo":{"status":"ok","timestamp":1618846622594,"user_tz":240,"elapsed":3232,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["import os\n","import tarfile\n","import glob\n","import io\n","import random\n","from tqdm import tqdm\n","import shutil\n","import tarfile\n","import PIL\n","from IPython.display import display, Image\n","import numpy as np\n","from six.moves.urllib.request import urlretrieve\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision import transforms"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KuMYUztgp6h_"},"source":["## Helper \n","# RGB -> YCbCr and YCbCr -> RGB image conversion\n","This part is taken from Kornia\n","https://kornia.readthedocs.io/en/latest/_modules/kornia/color/ycbcr.html"]},{"cell_type":"code","metadata":{"id":"ZJlq-WlfqQ69","executionInfo":{"status":"ok","timestamp":1618846638662,"user_tz":240,"elapsed":675,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["def rgb_to_ycbcr(image: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Convert an RGB image to YCbCr.\n","\n","    Args:\n","        image (torch.Tensor): RGB Image to be converted to YCbCr with shape :math:`(*, 3, H, W)`.\n","\n","    Returns:\n","        torch.Tensor: YCbCr version of the image with shape :math:`(*, 3, H, W)`.\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> output = rgb_to_ycbcr(input)  # 2x3x4x5\n","    \"\"\"\n","    if not isinstance(image, torch.Tensor):\n","        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n","            type(image)))\n","\n","    if len(image.shape) < 3 or image.shape[-3] != 3:\n","        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n","                         .format(image.shape))\n","\n","    r: torch.Tensor = image[..., 0, :, :]\n","    g: torch.Tensor = image[..., 1, :, :]\n","    b: torch.Tensor = image[..., 2, :, :]\n","\n","    delta: float = 0.5\n","    y: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n","    cb: torch.Tensor = (b - y) * 0.564 + delta\n","    cr: torch.Tensor = (r - y) * 0.713 + delta\n","    return torch.stack([y, cb, cr], -3)\n","\n","\n","\n","def ycbcr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Convert an YCbCr image to RGB.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Args:\n","        image (torch.Tensor): YCbCr Image to be converted to RGB with shape :math:`(*, 3, H, W)`.\n","\n","    Returns:\n","        torch.Tensor: RGB version of the image with shape :math:`(*, 3, H, W)`.\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> output = ycbcr_to_rgb(input)  # 2x3x4x5\n","    \"\"\"\n","    if not isinstance(image, torch.Tensor):\n","        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n","            type(image)))\n","\n","    if len(image.shape) < 3 or image.shape[-3] != 3:\n","        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n","                         .format(image.shape))\n","\n","    y: torch.Tensor = image[..., 0, :, :]\n","    cb: torch.Tensor = image[..., 1, :, :]\n","    cr: torch.Tensor = image[..., 2, :, :]\n","\n","    delta: float = 0.5\n","    cb_shifted: torch.Tensor = cb - delta\n","    cr_shifted: torch.Tensor = cr - delta\n","\n","    r: torch.Tensor = y + 1.403 * cr_shifted\n","    g: torch.Tensor = y - 0.714 * cr_shifted - 0.344 * cb_shifted\n","    b: torch.Tensor = y + 1.773 * cb_shifted\n","    return torch.stack([r, g, b], -3)\n","\n","\n","\n","class RgbToYcbcr(nn.Module):\n","    \"\"\"Convert an image from RGB to YCbCr.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Returns:\n","        torch.Tensor: YCbCr version of the image.\n","\n","    Shape:\n","        - image: :math:`(*, 3, H, W)`\n","        - output: :math:`(*, 3, H, W)`\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> ycbcr = RgbToYcbcr()\n","        >>> output = ycbcr(input)  # 2x3x4x5\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(RgbToYcbcr, self).__init__()\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        return rgb_to_ycbcr(image)\n","\n","\n","\n","class YcbcrToRgb(nn.Module):\n","    \"\"\"Convert an image from YCbCr to Rgb.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Returns:\n","        torch.Tensor: RGB version of the image.\n","\n","    Shape:\n","        - image: :math:`(*, 3, H, W)`\n","        - output: :math:`(*, 3, H, W)`\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> rgb = YcbcrToRgb()\n","        >>> output = rgb(input)  # 2x3x4x5\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(YcbcrToRgb, self).__init__()\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        return ycbcr_to_rgb(image)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eO181ezuqXK6"},"source":["## Directory setup and preparing the datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6cRBsi_0Z2s","executionInfo":{"status":"ok","timestamp":1618846658944,"user_tz":240,"elapsed":19713,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"8d44dee8-57d0-452c-c145-455f2abe9f58"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z9bnrCCa4Y_E","executionInfo":{"status":"ok","timestamp":1618846661126,"user_tz":240,"elapsed":691,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["directory = '/content/memnet'\n","if not os.path.exists(directory):\n","  os.makedirs(directory)\n","\n","os.chdir(directory)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uO1JLjItq4Pd"},"source":["## Model : Single-Supervised MemNet "]},{"cell_type":"code","metadata":{"id":"A5rUr_6uDYZg","executionInfo":{"status":"ok","timestamp":1618846663560,"user_tz":240,"elapsed":589,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class FeatExtBlock(nn.Module):\n","    def __init__(self, in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1):\n","        super(FeatExtBlock, self).__init__()\n","        self.feature = nn.Sequential(nn.BatchNorm2d(1), nn.ReLU(),\n","                                     nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n","                                               kernel_size=kernel_size, stride=stride,\n","                                               padding=padding, bias=False))\n","\n","    def forward(self, x):\n","        fe_out = self.feature(x)\n","        return fe_out"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoywv9Bk5HNp","executionInfo":{"status":"ok","timestamp":1618846663915,"user_tz":240,"elapsed":935,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class ResidualBlock(nn.Module):\n","  def __init__(self, n_layers=2, n_channels=64, kernel_size=3, stride=1, padding=1):\n","    super(ResidualBlock, self).__init__()\n","    layers = []\n","    for _ in range(n_layers):\n","      layers.append(nn.BatchNorm2d(n_channels))\n","      layers.append(nn.ReLU())\n","      layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n","                              stride=stride, padding=padding, bias=False))\n","    \n","    self.residual = nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    out = self.residual(x)\n","    return out + x"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"awmHxvKb69Xl","executionInfo":{"status":"ok","timestamp":1618846663915,"user_tz":240,"elapsed":928,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class RecursiveUnit(nn.Module):\n","  def __init__(self, n_short=6, d_conv=64):\n","    super(RecursiveUnit, self).__init__()\n","    self.n_short = n_short\n","    self.residual = nn.ModuleList([ResidualBlock(n_layers=2, n_channels=d_conv) for _ in range(n_short)])\n","\n","  def forward(self, b_prev_m):\n","    Hs = []\n","    Hs.append(self.residual[0](b_prev_m))\n","    for rec in range(1, self.n_short):\n","      Hs.append(self.residual[rec](Hs[-1]))\n","    b_short = torch.cat(Hs, dim=1)\n","    return b_short"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CqUd6dgpRBgA","executionInfo":{"status":"ok","timestamp":1618846663916,"user_tz":240,"elapsed":921,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class GateUnit(nn.Module):\n","    def __init__(self, in_channels, d_conv=64):\n","        super(GateUnit, self).__init__()\n","        self.in_channels = in_channels\n","        self.gate_layer = nn.Sequential(nn.BatchNorm2d(in_channels), nn.ReLU(),\n","                                  nn.Conv2d(in_channels=self.in_channels, out_channels=d_conv, kernel_size=1,\n","                                            stride=1, padding=0, bias=False))\n","\n","    def forward(self, b_gate):\n","        b_mem = self.gate_layer(b_gate)\n","        return b_mem"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8MftIwz-CIk","executionInfo":{"status":"ok","timestamp":1618846663916,"user_tz":240,"elapsed":914,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class MemBlock(nn.Module):\n","  def __init__(self, n_long, N_short=6, d_conv=64):\n","    super(MemBlock, self).__init__()\n","    self.n_long = n_long\n","    self.N_long = N_short\n","    self.d_conv = d_conv\n","    self.gate_in = (n_long + N_short) * d_conv\n","    self.short_unit = RecursiveUnit(N_short, d_conv)\n","    self.gate = GateUnit(self.gate_in, self.d_conv)\n","\n","  def forward(self, b_long):\n","    b_short = self.short_unit(b_long[:, -self.d_conv:, :, :])\n","    b_gate = torch.cat([b_short, b_long], dim=1)\n","    b_m = self.gate(b_gate)\n","    return b_m"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZVQ3ZRZFLNj","executionInfo":{"status":"ok","timestamp":1618846663917,"user_tz":240,"elapsed":907,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class ReconstBlock(nn.Module):\n","  def __init__(self, d_conv=64, kernel_size=3, stride=1, padding=1):\n","    super(ReconstBlock, self).__init__()\n","    self.d_conv = d_conv\n","    self.recon = nn.Sequential(nn.BatchNorm2d(d_conv), nn.ReLU(),\n","                               nn.Conv2d(in_channels=d_conv, out_channels=1, kernel_size=kernel_size,\n","                                         stride=stride, padding=padding, bias=False))\n","\n","  def forward(self, b_m, x):\n","    y = self.recon(b_m)\n","    return y+x"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEyTxqG0xfAK","executionInfo":{"status":"ok","timestamp":1618846747451,"user_tz":240,"elapsed":717,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class MemNet_MS(nn.Module):\n","    def __init__(self, N_long, N_short, d_conv=64):\n","        super(MemNet_MS, self).__init__()\n","        self.N_long = N_long\n","        self.N_short = N_short\n","        self.d_conv = d_conv\n","        self.fe_block = FeatExtBlock(1, d_conv)\n","        self.mem_blocks = nn.ModuleList([MemBlock(n_mem+1, N_short, d_conv) for n_mem in range(N_long)])\n","        self.recon_block = ReconstBlock(d_conv)\n","        # This part is taken from: https://discuss.pytorch.org/t/contraining-weights-to-sum-to-1/20609/2\n","        self.eps = 1E-7\n","        self.linavg = nn.Parameter((1./self.N_long) * torch.ones(N_long), requires_grad=True)\n","\n","    def forward(self, x):\n","      fe = self.fe_block(x)\n","      b_ms = []\n","      b_ms.append(fe)\n","      ys = []\n","      y = torch.zeros_like(x)\n","      for n_mem in range(self.N_long):\n","        b_long = torch.cat(b_ms, dim=1)\n","        b_ms.append(self.mem_blocks[n_mem](b_long))\n","        # self.linavg = self.linavg.clamp(min=self.eps) # in case weights > 0\n","        ys.append(self.recon_block(b_ms[-1], x))\n","        linavg_sum = self.linavg.sum(0, keepdim=True)\n","        y += self.linavg[n_mem] * ys[-1] / linavg_sum\n","      \n","      # y = self.recon_block(b_ms[-1], x)\n","      return y, ys"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sszPhXlL1B6b","executionInfo":{"status":"ok","timestamp":1618846751607,"user_tz":240,"elapsed":846,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"012e9685-0e1d-4dfc-ff70-c9e3212832ee"},"source":["def size_check():\n","  p = torch.rand(2, 1, 4, 4)\n","  print(p.size())\n","  d_conv = 8\n","  fe_model = FeatExtBlock(1, d_conv)\n","  q = fe_model(p)\n","  print(q.size())\n","  mem_model = MemBlock(1, 6, d_conv)\n","  r = mem_model(q)\n","  print(r.size())\n","  recon_model = ReconstBlock(d_conv)\n","  s = recon_model(r, p)\n","  print(s.size())\n","  model = MemNet_MS(2, 2, 32)\n","  t, q = model(p)\n","  print(t.size())\n","  print(len(q))\n","\n","size_check()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["torch.Size([2, 1, 4, 4])\n","torch.Size([2, 8, 4, 4])\n","torch.Size([2, 8, 4, 4])\n","torch.Size([2, 1, 4, 4])\n","torch.Size([2, 1, 4, 4])\n","2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zkrs29MLquvQ"},"source":["## Dataset Preparation"]},{"cell_type":"code","metadata":{"id":"6R5hB9SzAzpi","executionInfo":{"status":"ok","timestamp":1618846788367,"user_tz":240,"elapsed":547,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class DatasetSR(Dataset):\n","  def __init__(self, images_dir, transform=None):\n","    self.files = []\n","    for dir in images_dir:\n","      self.files += glob.glob(dir + '/*')\n","    self.files = sorted(self.files)\n","    self.transform=transform\n","  \n","  def __len__(self):\n","    return len(self.files)\n","\n","  def __getitem__(self, index):\n","    image_path = self.files[index]\n","    hr_image = PIL.Image.open(image_path)\n","    hr_image = hr_image.convert('YCbCr')\n","    # random_gen = int(image_path.split('/')[-1].split('_')[-2][-1]) + int(image_path.split('/')[-1].split('_')[-1][-5])\n","    # scale = random_gen % 3 + 2 \n","    scale = random.randint(2, 4)\n","    width_down = hr_image.width // scale\n","    heigth_down = hr_image.height // scale\n","    lr_image = hr_image.resize((width_down, heigth_down),\n","                               resample=PIL.Image.BICUBIC)\n","    lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                               resample=PIL.Image.BICUBIC)\n","    \n","    trans_toten = transforms.ToTensor()\n","    hr_ten = trans_toten(hr_image)\n","    lr_ten = trans_toten(lr_image)\n","    hr_ten = rgb_to_ycbcr(hr_ten)\n","    lr_ten = rgb_to_ycbcr(lr_ten)\n","    \n","    if self.transform:\n","            hr = self.transform(hr_image)\n","            lr = self.transform(lr_image)\n","\n","    return hr, lr"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_mQTJeIrHfl"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"2r1QT0atLrnY","executionInfo":{"status":"ok","timestamp":1618846790557,"user_tz":240,"elapsed":522,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class AttrDict(dict):\n","  def __init__(self, *args, **kwargs):\n","    super(AttrDict, self).__init__(*args, **kwargs)\n","    self.__dict__ = self\n","\n","def weights_init(m):\n","  if isinstance(m, nn.Conv2d):\n","    nn.init.xavier_normal_(m.weight.data, gain=1.0)\n","    # nn.init.zeros_(m.bias.data)\n","\n","# The following function si taken from: https://medium.com/analytics-vidhya/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61\n","def load_ckp(checkpoint_fpath, model):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    model.load_state_dict(checkpoint)\n","    # optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model\n","\n","def train(args):\n","  model = MemNet_SS(args.N_long, args.N_short, args.d_conv)\n","  model.train()\n","  if torch.cuda.is_available():\n","    print(\"sending model to cuda...\")\n","    model = model.cuda()\n","  # optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","  optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.w_decay)\n","  if args['start_from']:\n","    print('starting from {}'.format(args.start_from))\n","    checkpoint = torch.load(args.start_from)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    # model = load_ckp(args['start_from'], model)\n","  else:\n","    print(\"initializing the model...\")\n","    model.apply(weights_init)\n","  \n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.epochs//8, gamma=args.gamma)\n","  criterion = nn.MSELoss(reduction='sum')\n","  trnsfrm = transforms.Compose([transforms.ToTensor()])\n","  # dataset = Dataset(opt.images_dir, opt.patch_size, opt.jpeg_quality, opt.use_fast_loader)\n","  dataset = DatasetSR(images_dir=args.train_dir, transform=trnsfrm)\n","  dataloader = DataLoader(dataset=dataset,\n","                          batch_size=args.batch_size,\n","                          shuffle=True,\n","                          num_workers=args.threads,\n","                          pin_memory=True,\n","                          drop_last=False)\n","  \n","  print('Training starts...')\n","  alpha_loss = 1.0 / (1.+args.N_long)\n","  agg_coeff = 0.5*alpha_loss/args.batch_size\n","  mem_coeff = 0.5*(1.0-alpha_loss)/(args.N_long*args.batch_size)\n","  start = time.time()\n","  train_losses = []\n","  valid_losses = []\n","  for epoch in range(args.epochs):\n","    losses = []\n","    for i, data in enumerate(dataloader):\n","      y, x = data\n","      x = x[:, 0, :, :].unsqueeze(1)\n","      y = y[:, 0, :, :].unsqueeze(1)\n","      if torch.cuda.is_available():\n","        x = x.cuda()\n","        y = y.cuda()\n","      \n","      x_hat, xs = model(x)\n","      # print(\"evaluated\")\n","      loss = agg_coeff*criterion(x_hat, y)\n","      for x_i in xs:\n","        loss += mem_coeff*criterion(x_i, y)\n","      \n","      optimizer.zero_grad()\n","      loss.backward()\n","      nn.utils.clip_grad_norm_(model.parameters(), args.clip) \n","      optimizer.step()\n","      losses.append(loss.data.item())\n","\n","    # print(\"moving forward...\")\n","    scheduler.step()\n","    avg_loss = np.mean(losses)\n","    train_losses.append(avg_loss)\n","    time_elapsed = time.time() - start\n","    print('Epoch [%d/%d], Loss: %.4f, Time (s): %d'\n","          % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n","        )\n","    if (epoch+1) % 5 == 0:\n","      patch_checkpoint = os.path.join(args.output_dir, 'memnet_epoch_{}.pt'.format(epoch+1))\n","      torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, patch_checkpoint)\n","      # torch.save(model.state_dict(), os.path.join(args.output_dir, 'memnet_epoch_{}.pt'.format(epoch+1)))\n","\n","  # Plot training curve\n","  plt.figure()\n","  plt.plot(train_losses, \"ro-\", label=\"Train\")\n","  # plt.plot(valid_losses, \"go-\", label=\"Validation\")\n","  plt.legend()\n","  plt.title(\"Loss\")\n","  plt.xlabel(\"Epochs\")\n","  plt.show()\n","  plt.savefig(os.path.join(args.output_dir, \"train_loss.png\"))\n","  \n","  return model"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfRGHdI1PPQu","executionInfo":{"status":"ok","timestamp":1618846802705,"user_tz":240,"elapsed":740,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["drive_dir = '/content/drive/MyDrive/memnet/memnet_ms_v3'\n","if not os.path.exists(drive_dir):\n","  os.makedirs(drive_dir)\n","\n","patch_size = 31\n","trainings = ['BSDS200_p'+str(patch_size), 'T91_p'+str(patch_size), 'General100_p'+str(patch_size)]\n","args = AttrDict()\n","args_dict = {\n","    'threads' : 1,\n","    'kernel_size' : 3,\n","    'N_long' : 6,\n","    'N_short' : 6,\n","    'd_conv' : 64,\n","    'lr' : 0.1,\n","    'momentum' : 0.9,\n","    'w_decay' : 1E-4,\n","    'clip' : 0.4,\n","    'gamma' : 0.2,\n","    'batch_size':128,\n","    'patch_size': patch_size,\n","    'epochs' : 80,\n","    'train_dir' : [directory+'/'+trainings[i] for i in range(len(trainings))],\n","    'output_dir' : drive_dir,\n","    'start_from' : ''\n","}\n","args.update(args_dict)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCGLKe5dTbNk","executionInfo":{"status":"ok","timestamp":1618846839375,"user_tz":240,"elapsed":28784,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"379c3164-a236-4abd-937f-e999afdd0a83"},"source":["gdrive_tdir = '/content/drive/MyDrive/memnet'\n","for tri in trainings:\n","  shutil.copy(os.path.join(gdrive_tdir, tri + '.tar.gz'), directory)\n","  file = tarfile.open(os.path.join(directory, tri + '.tar.gz'))\n","  file.extractall('./')\n","  file.close()\n","  print(len(next(os.walk('./'+tri))[2]))\n","\n","# !ls /content/drive/MyDrive/memnet\n","# !cp /content/drive/MyDrive/memnet/T91_p51.tar.gz /content/memnet/T91_p51.tar.gz\n","# with tarfile.open('T91_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !cp /content/drive/MyDrive/memnet/BSDS200_p51.tar.gz /content/memnet/BSDS200_p51.tar.gz\n","# with tarfile.open('BSDS200_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !cp /content/drive/MyDrive/memnet/General100_p51.tar.gz /content/memnet/General100_p51.tar.gz\n","# with tarfile.open('General100_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !ls /content/memnet/T91_p51/ -1 | wc -l\n","# !ls /content/memnet/BSDS200_p51/ -1 | wc -l\n","# !ls /content/memnet/General100_p31/ -1 | wc -l"],"execution_count":21,"outputs":[{"output_type":"stream","text":["73600\n","13115\n","42803\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uoGJUelbTfGc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1618889683754,"user_tz":240,"elapsed":42841846,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"80e41570-3c07-451a-89c9-6e7dc6687baf"},"source":["memnet = train(args)\n","while True:pass"],"execution_count":22,"outputs":[{"output_type":"stream","text":["sending model to cuda...\n","initializing the model...\n","Training starts...\n","Epoch [1/80], Loss: 6.8573, Time (s): 531\n","Epoch [2/80], Loss: 1.3519, Time (s): 1062\n","Epoch [3/80], Loss: 1.1742, Time (s): 1594\n","Epoch [4/80], Loss: 1.1290, Time (s): 2125\n","Epoch [5/80], Loss: 1.1126, Time (s): 2657\n","Epoch [6/80], Loss: 1.1049, Time (s): 3189\n","Epoch [7/80], Loss: 1.1029, Time (s): 3720\n","Epoch [8/80], Loss: 1.0885, Time (s): 4252\n","Epoch [9/80], Loss: 1.0853, Time (s): 4783\n","Epoch [10/80], Loss: 1.0886, Time (s): 5314\n","Epoch [11/80], Loss: 1.0531, Time (s): 5844\n","Epoch [12/80], Loss: 1.0527, Time (s): 6375\n","Epoch [13/80], Loss: 1.0469, Time (s): 6906\n","Epoch [14/80], Loss: 1.0440, Time (s): 7437\n","Epoch [15/80], Loss: 1.0412, Time (s): 7968\n","Epoch [16/80], Loss: 1.0418, Time (s): 8499\n","Epoch [17/80], Loss: 1.0407, Time (s): 9030\n","Epoch [18/80], Loss: 1.0414, Time (s): 9562\n","Epoch [19/80], Loss: 1.0384, Time (s): 10093\n","Epoch [20/80], Loss: 1.0321, Time (s): 10624\n","Epoch [21/80], Loss: 1.0249, Time (s): 11156\n","Epoch [22/80], Loss: 1.0258, Time (s): 11688\n","Epoch [23/80], Loss: 1.0194, Time (s): 12220\n","Epoch [24/80], Loss: 1.0183, Time (s): 12751\n","Epoch [25/80], Loss: 1.0159, Time (s): 13283\n","Epoch [26/80], Loss: 1.0190, Time (s): 13815\n","Epoch [27/80], Loss: 1.0212, Time (s): 14347\n","Epoch [28/80], Loss: 1.0172, Time (s): 14879\n","Epoch [29/80], Loss: 1.0199, Time (s): 15411\n","Epoch [30/80], Loss: 1.0173, Time (s): 15942\n","Epoch [31/80], Loss: 1.0113, Time (s): 16474\n","Epoch [32/80], Loss: 1.0153, Time (s): 17005\n","Epoch [33/80], Loss: 1.0138, Time (s): 17537\n","Epoch [34/80], Loss: 1.0127, Time (s): 18069\n","Epoch [35/80], Loss: 1.0110, Time (s): 18601\n","Epoch [36/80], Loss: 1.0118, Time (s): 19133\n","Epoch [37/80], Loss: 1.0139, Time (s): 19665\n","Epoch [38/80], Loss: 1.0142, Time (s): 20196\n","Epoch [39/80], Loss: 1.0128, Time (s): 20727\n","Epoch [40/80], Loss: 1.0154, Time (s): 21258\n","Epoch [41/80], Loss: 1.0101, Time (s): 21790\n","Epoch [42/80], Loss: 1.0148, Time (s): 22321\n","Epoch [43/80], Loss: 1.0106, Time (s): 22852\n","Epoch [44/80], Loss: 1.0103, Time (s): 23383\n","Epoch [45/80], Loss: 1.0112, Time (s): 23914\n","Epoch [46/80], Loss: 1.0148, Time (s): 24445\n","Epoch [47/80], Loss: 1.0120, Time (s): 24976\n","Epoch [48/80], Loss: 1.0106, Time (s): 25507\n","Epoch [49/80], Loss: 1.0128, Time (s): 26038\n","Epoch [50/80], Loss: 1.0108, Time (s): 26569\n","Epoch [51/80], Loss: 1.0094, Time (s): 27101\n","Epoch [52/80], Loss: 1.0127, Time (s): 27631\n","Epoch [53/80], Loss: 1.0053, Time (s): 28162\n","Epoch [54/80], Loss: 1.0116, Time (s): 28693\n","Epoch [55/80], Loss: 1.0085, Time (s): 29224\n","Epoch [56/80], Loss: 1.0101, Time (s): 29755\n","Epoch [57/80], Loss: 1.0133, Time (s): 30286\n","Epoch [58/80], Loss: 1.0119, Time (s): 30817\n","Epoch [59/80], Loss: 1.0091, Time (s): 31349\n","Epoch [60/80], Loss: 1.0135, Time (s): 31880\n","Epoch [61/80], Loss: 1.0091, Time (s): 32411\n","Epoch [62/80], Loss: 1.0085, Time (s): 32942\n","Epoch [63/80], Loss: 1.0096, Time (s): 33474\n","Epoch [64/80], Loss: 1.0127, Time (s): 34005\n","Epoch [65/80], Loss: 1.0123, Time (s): 34536\n","Epoch [66/80], Loss: 1.0128, Time (s): 35068\n","Epoch [67/80], Loss: 1.0087, Time (s): 35599\n","Epoch [68/80], Loss: 1.0111, Time (s): 36130\n","Epoch [69/80], Loss: 1.0102, Time (s): 36661\n","Epoch [70/80], Loss: 1.0070, Time (s): 37193\n","Epoch [71/80], Loss: 1.0098, Time (s): 37724\n","Epoch [72/80], Loss: 1.0082, Time (s): 38255\n","Epoch [73/80], Loss: 1.0097, Time (s): 38787\n","Epoch [74/80], Loss: 1.0094, Time (s): 39318\n","Epoch [75/80], Loss: 1.0111, Time (s): 39849\n","Epoch [76/80], Loss: 1.0106, Time (s): 40380\n","Epoch [77/80], Loss: 1.0139, Time (s): 40912\n","Epoch [78/80], Loss: 1.0146, Time (s): 41443\n","Epoch [79/80], Loss: 1.0099, Time (s): 41974\n","Epoch [80/80], Loss: 1.0074, Time (s): 42505\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-b289ffe118e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmemnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]}]}