{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "memnet_multi_supervised.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KuMYUztgp6h_"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS9_H89C0o1b"
      },
      "source": [
        "## Module setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khmWyAd17fbU"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import glob\n",
        "import io\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import PIL\n",
        "from IPython.display import display, Image\n",
        "\n",
        "import numpy as np\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "# import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "# from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuMYUztgp6h_"
      },
      "source": [
        "## Helper \n",
        "# RGB -> YCbCr and YCbCr -> RGB image conversion\n",
        "This part is taken from Kornia\n",
        "https://kornia.readthedocs.io/en/latest/_modules/kornia/color/ycbcr.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJlq-WlfqQ69"
      },
      "source": [
        "def rgb_to_ycbcr(image: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Convert an RGB image to YCbCr.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): RGB Image to be converted to YCbCr with shape :math:`(*, 3, H, W)`.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: YCbCr version of the image with shape :math:`(*, 3, H, W)`.\n",
        "\n",
        "    Examples:\n",
        "        >>> input = torch.rand(2, 3, 4, 5)\n",
        "        >>> output = rgb_to_ycbcr(input)  # 2x3x4x5\n",
        "    \"\"\"\n",
        "    if not isinstance(image, torch.Tensor):\n",
        "        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n",
        "            type(image)))\n",
        "\n",
        "    if len(image.shape) < 3 or image.shape[-3] != 3:\n",
        "        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n",
        "                         .format(image.shape))\n",
        "\n",
        "    r: torch.Tensor = image[..., 0, :, :]\n",
        "    g: torch.Tensor = image[..., 1, :, :]\n",
        "    b: torch.Tensor = image[..., 2, :, :]\n",
        "\n",
        "    delta: float = 0.5\n",
        "    y: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "    cb: torch.Tensor = (b - y) * 0.564 + delta\n",
        "    cr: torch.Tensor = (r - y) * 0.713 + delta\n",
        "    return torch.stack([y, cb, cr], -3)\n",
        "\n",
        "\n",
        "\n",
        "def ycbcr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Convert an YCbCr image to RGB.\n",
        "\n",
        "    The image data is assumed to be in the range of (0, 1).\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): YCbCr Image to be converted to RGB with shape :math:`(*, 3, H, W)`.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: RGB version of the image with shape :math:`(*, 3, H, W)`.\n",
        "\n",
        "    Examples:\n",
        "        >>> input = torch.rand(2, 3, 4, 5)\n",
        "        >>> output = ycbcr_to_rgb(input)  # 2x3x4x5\n",
        "    \"\"\"\n",
        "    if not isinstance(image, torch.Tensor):\n",
        "        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n",
        "            type(image)))\n",
        "\n",
        "    if len(image.shape) < 3 or image.shape[-3] != 3:\n",
        "        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n",
        "                         .format(image.shape))\n",
        "\n",
        "    y: torch.Tensor = image[..., 0, :, :]\n",
        "    cb: torch.Tensor = image[..., 1, :, :]\n",
        "    cr: torch.Tensor = image[..., 2, :, :]\n",
        "\n",
        "    delta: float = 0.5\n",
        "    cb_shifted: torch.Tensor = cb - delta\n",
        "    cr_shifted: torch.Tensor = cr - delta\n",
        "\n",
        "    r: torch.Tensor = y + 1.403 * cr_shifted\n",
        "    g: torch.Tensor = y - 0.714 * cr_shifted - 0.344 * cb_shifted\n",
        "    b: torch.Tensor = y + 1.773 * cb_shifted\n",
        "    return torch.stack([r, g, b], -3)\n",
        "\n",
        "\n",
        "\n",
        "class RgbToYcbcr(nn.Module):\n",
        "    \"\"\"Convert an image from RGB to YCbCr.\n",
        "\n",
        "    The image data is assumed to be in the range of (0, 1).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: YCbCr version of the image.\n",
        "\n",
        "    Shape:\n",
        "        - image: :math:`(*, 3, H, W)`\n",
        "        - output: :math:`(*, 3, H, W)`\n",
        "\n",
        "    Examples:\n",
        "        >>> input = torch.rand(2, 3, 4, 5)\n",
        "        >>> ycbcr = RgbToYcbcr()\n",
        "        >>> output = ycbcr(input)  # 2x3x4x5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super(RgbToYcbcr, self).__init__()\n",
        "\n",
        "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        return rgb_to_ycbcr(image)\n",
        "\n",
        "\n",
        "\n",
        "class YcbcrToRgb(nn.Module):\n",
        "    \"\"\"Convert an image from YCbCr to Rgb.\n",
        "\n",
        "    The image data is assumed to be in the range of (0, 1).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: RGB version of the image.\n",
        "\n",
        "    Shape:\n",
        "        - image: :math:`(*, 3, H, W)`\n",
        "        - output: :math:`(*, 3, H, W)`\n",
        "\n",
        "    Examples:\n",
        "        >>> input = torch.rand(2, 3, 4, 5)\n",
        "        >>> rgb = YcbcrToRgb()\n",
        "        >>> output = rgb(input)  # 2x3x4x5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super(YcbcrToRgb, self).__init__()\n",
        "\n",
        "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        return ycbcr_to_rgb(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO181ezuqXK6"
      },
      "source": [
        "## Directory setup and preparing the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6cRBsi_0Z2s",
        "outputId": "1a869e0b-478a-4c7d-8bc4-a620c733e90c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9bnrCCa4Y_E"
      },
      "source": [
        "drive_dir = '/content/drive/MyDrive/memnet/memnet_ms_t91'\n",
        "if not os.path.exists(drive_dir):\n",
        "  os.makedirs(drive_dir)\n",
        "\n",
        "directory = '/content/memnet'\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "os.chdir(directory)\n",
        "# %mkdir -p /content/drive/MyDrive/memnet\n",
        "# %cd /content/drive/MyDrive/memnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcghdQBZzYhe",
        "outputId": "86642a0f-0413-4217-8228-8ba6766ceff6"
      },
      "source": [
        "!ls /content/drive/MyDrive/memnet\n",
        "!cp /content/drive/MyDrive/memnet/t91_crop_training.tar.gz /content/memnet/t91_crop_training.tar.gz\n",
        "with tarfile.open('t91_crop_training.tar.gz') as archive:\n",
        "  archive.extractall(directory)\n",
        "\n",
        "!ls /content/memnet/t91_crop_training/ -1 | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BSDS300-images.tgz    memnet_ms_bsds  memnet_ss_bsds  t91_crop_training.tar.gz\n",
            "crop_training.tar.gz  memnet_ms_t91   sample_tests\n",
            "ls: cannot access '/content/memnet/crop_training/': No such file or directory\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAC9_hm-gqUr"
      },
      "source": [
        "# bsds = 'https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz'\n",
        "# bsds_file = directory+'/bsds300'\n",
        "# urlretrieve(bsds, bsds_file)\n",
        "# with tarfile.open(bsds_file) as archive:\n",
        "#   archive.extractall(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO1JLjItq4Pd"
      },
      "source": [
        "## Model : Multi-Supervised MemNet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5rUr_6uDYZg"
      },
      "source": [
        "class FeatExtBlock(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1):\n",
        "        super(FeatExtBlock, self).__init__()\n",
        "        self.feature = nn.Sequential(nn.BatchNorm2d(in_channels), nn.ReLU(),\n",
        "                                     nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
        "                                               kernel_size=kernel_size, stride=stride,\n",
        "                                               padding=padding, bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fe_out = self.feature(x)\n",
        "        return fe_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoywv9Bk5HNp"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, n_layers=2, n_channels=32, kernel_size=3, stride=1, padding=1):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    layers = []\n",
        "    for _ in range(n_layers):\n",
        "      layers.append(nn.BatchNorm2d(n_channels))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
        "                              stride=stride, padding=padding, bias=False))\n",
        "    \n",
        "    self.residual = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.residual(x)\n",
        "    return out + x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awmHxvKb69Xl"
      },
      "source": [
        "class RecursiveUnit(nn.Module):\n",
        "  def __init__(self, n_short=6, d_conv=32):\n",
        "    super(RecursiveUnit, self).__init__()\n",
        "    self.n_short = n_short\n",
        "    self.residual = nn.ModuleList([ResidualBlock(n_layers=2, n_channels=d_conv) for _ in range(n_short)])\n",
        "\n",
        "  def forward(self, b_prev_m):\n",
        "    Hs = []\n",
        "    Hs.append(self.residual[0](b_prev_m))\n",
        "    for rec in range(1, self.n_short):\n",
        "      Hs.append(self.residual[rec](Hs[-1]))\n",
        "    b_short = torch.cat(Hs, dim=1)\n",
        "    return b_short"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqUd6dgpRBgA"
      },
      "source": [
        "class GateUnit(nn.Module):\n",
        "    def __init__(self, in_channels, d_conv=32):\n",
        "        super(GateUnit, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        \n",
        "        self.gate_layer = nn.Sequential(nn.BatchNorm2d(in_channels), nn.ReLU(),\n",
        "                                  nn.Conv2d(in_channels=self.in_channels, out_channels=d_conv, kernel_size=1,\n",
        "                                            stride=1, padding=0, bias=False))\n",
        "\n",
        "    def forward(self, b_gate):\n",
        "        b_mem = self.gate_layer(b_gate)\n",
        "        return b_mem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8MftIwz-CIk"
      },
      "source": [
        "class MemBlock(nn.Module):\n",
        "  def __init__(self, n_long, N_short=6, d_conv=32):\n",
        "    super(MemBlock, self).__init__()\n",
        "    self.n_long = n_long\n",
        "    self.N_long = N_short\n",
        "    self.d_conv = d_conv\n",
        "    self.gate_in = (n_long + N_short) * d_conv\n",
        "    self.short_unit = RecursiveUnit(N_short, d_conv)\n",
        "    self.gate = GateUnit(self.gate_in, self.d_conv)\n",
        "\n",
        "  def forward(self, b_long):\n",
        "    b_short = self.short_unit(b_long[:, -self.d_conv:, :, :])\n",
        "    b_gate = torch.cat([b_short, b_long], dim=1)\n",
        "    b_m = self.gate(b_gate)\n",
        "    return b_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZVQ3ZRZFLNj"
      },
      "source": [
        "class ReconstBlock(nn.Module):\n",
        "  def __init__(self, d_conv=32, kernel_size=3, stride=1, padding=1):\n",
        "    super(ReconstBlock, self).__init__()\n",
        "    self.d_conv = d_conv\n",
        "    self.recon = nn.Sequential(nn.BatchNorm2d(d_conv),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Conv2d(in_channels=d_conv, out_channels=1, kernel_size=kernel_size,\n",
        "                                         stride=stride, padding=padding, bias=False))\n",
        "\n",
        "  def forward(self, b_m, x):\n",
        "    y = self.recon(b_m)\n",
        "    return y+x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEyTxqG0xfAK"
      },
      "source": [
        "class MemNet(nn.Module):\n",
        "    def __init__(self, N_long, N_short, d_conv=32):\n",
        "        super(MemNet, self).__init__()\n",
        "        self.N_long = N_long\n",
        "        self.N_short = N_short\n",
        "        self.d_conv = d_conv\n",
        "        self.fe_block = FeatExtBlock(1, d_conv)\n",
        "        self.mem_blocks = nn.ModuleList([MemBlock(n_mem+1, N_short, d_conv) for n_mem in range(N_long)])\n",
        "        self.recon_block = ReconstBlock(d_conv)\n",
        "        # This part is taken from: https://discuss.pytorch.org/t/contraining-weights-to-sum-to-1/20609/2\n",
        "        self.eps = 1E-7\n",
        "        self.linavg = nn.Parameter((1./self.N_long) * torch.ones(N_long), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "      fe = self.fe_block(x)\n",
        "      b_ms = []\n",
        "      b_ms.append(fe)\n",
        "      y = torch.zeros_like(x)\n",
        "      for n_mem in range(self.N_long):\n",
        "        b_long = torch.cat(b_ms, dim=1)\n",
        "        b_ms.append(self.mem_blocks[n_mem](b_long))\n",
        "        # self.linavg = self.linavg.clamp(min=self.eps) # in case weights > 0\n",
        "        linavg_sum = self.linavg.sum(0, keepdim=True)\n",
        "        y += self.linavg[n_mem] * self.recon_block(b_ms[-1], x) / linavg_sum\n",
        "\n",
        "      return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sszPhXlL1B6b",
        "outputId": "ec2d24b1-e937-4bad-c6d9-339cabf05fb6"
      },
      "source": [
        "def size_check():\n",
        "  p = torch.rand(2, 1, 4, 4)\n",
        "  print(p.size())\n",
        "  d_conv = 8\n",
        "  fe_model = FeatExtBlock(1, d_conv)\n",
        "  q = fe_model(p)\n",
        "  print(q.size())\n",
        "  mem_model = MemBlock(1, 6, d_conv)\n",
        "  r = mem_model(q)\n",
        "  print(r.size())\n",
        "  recon_model = ReconstBlock(d_conv)\n",
        "  s = recon_model(r, p)\n",
        "  print(s.size())\n",
        "  model = MemNet(2, 2, 32)\n",
        "  t = model(p)\n",
        "  print(t.size())\n",
        "\n",
        "size_check()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 4, 4])\n",
            "torch.Size([2, 8, 4, 4])\n",
            "torch.Size([2, 8, 4, 4])\n",
            "torch.Size([2, 1, 4, 4])\n",
            "torch.Size([2, 1, 4, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkrs29MLquvQ"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R5hB9SzAzpi"
      },
      "source": [
        "class DatasetSR(Dataset):\n",
        "  def __init__(self, images_dir, scale=None, patch_size=None, stride=21, transform=None):\n",
        "    self.dir = images_dir\n",
        "    self.files = sorted(glob.glob(images_dir + '/*'))\n",
        "    self.scale = scale\n",
        "    self.patch = patch_size\n",
        "    self.stride = stride\n",
        "    self.transform=transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = os.path.join(self.dir, self.files[index])\n",
        "\n",
        "    hr_image = PIL.Image.open(image_path)\n",
        "    hr_image = hr_image.convert('YCbCr')\n",
        "    \n",
        "    random_gen = int(image_path.split('/')[-1].split('_')[-2][-1]) + int(image_path.split('/')[-1].split('_')[-1][-5])\n",
        "    scale = random_gen % 3 + 2 #random.randint(2, 4)\n",
        "\n",
        "    width_down = self.patch // scale\n",
        "    heigth_down = self.patch // scale\n",
        "    lr_image = hr_image.resize((width_down, heigth_down),\n",
        "                               resample=PIL.Image.BICUBIC)\n",
        "    lr_image = lr_image.resize((hr_image.width, hr_image.height),\n",
        "                               resample=PIL.Image.BICUBIC)\n",
        "    \n",
        "    trans_toten = transforms.ToTensor()\n",
        "    hr_ten = trans_toten(hr_image)\n",
        "    lr_ten = trans_toten(lr_image)\n",
        "    hr_ten = rgb_to_ycbcr(hr_ten)\n",
        "    lr_ten = rgb_to_ycbcr(lr_ten)\n",
        "    \n",
        "    if self.transform:\n",
        "            hr = self.transform(hr_image)\n",
        "            lr = self.transform(lr_image)\n",
        "\n",
        "    return hr, lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_mQTJeIrHfl"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r1QT0atLrnY"
      },
      "source": [
        "class AttrDict(dict):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(AttrDict, self).__init__(*args, **kwargs)\n",
        "    self.__dict__ = self\n",
        "\n",
        "def weights_init(m):\n",
        "  if isinstance(m, nn.Conv2d):\n",
        "    nn.init.xavier_normal_(m.weight.data, gain=1.0)\n",
        "    # nn.init.zeros_(m.bias.data)\n",
        "\n",
        "# The following function si taken from: https://medium.com/analytics-vidhya/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61\n",
        "def load_ckp(checkpoint_fpath, model):\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model\n",
        "\n",
        "def train(args):\n",
        "  model = MemNet(args.N_long, args.N_short, args.d_conv)\n",
        "  model.train()\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "  \n",
        "  # optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.w_decay)\n",
        "  if args['start_from']:\n",
        "    print('starting from ')\n",
        "    model = load_ckp(args['start_from'], model)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.epochs//4, gamma=0.1)\n",
        "  criterion = nn.MSELoss()\n",
        "  # model.apply(weights_init)\n",
        "  trnsfrm = transforms.Compose([transforms.ToTensor()])\n",
        "  # dataset = Dataset(opt.images_dir, opt.patch_size, opt.jpeg_quality, opt.use_fast_loader)\n",
        "  dataset = DatasetSR(images_dir=args.train_dir, scale=args.scale, patch_size=args.patch_size, transform=trnsfrm)\n",
        "  dataloader = DataLoader(dataset=dataset,\n",
        "                          batch_size=args.batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=args.threads,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=False)\n",
        "  \n",
        "  print('Training starts...')\n",
        "  start = time.time()\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  for epoch in range(args.epochs):\n",
        "    losses = []\n",
        "    for i, data in enumerate(dataloader):\n",
        "      y, x = data\n",
        "      x = x[:, 0, :, :].unsqueeze(1)\n",
        "      y = y[:, 0, :, :].unsqueeze(1)\n",
        "      if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "      \n",
        "      x_hat = model(x)\n",
        "\n",
        "      loss = 0.5*args.patch_size*criterion(x_hat, y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), args.clip) \n",
        "      optimizer.step()\n",
        "      losses.append(loss.data.item())\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = np.mean(losses)\n",
        "    train_losses.append(avg_loss)\n",
        "    time_elapsed = time.time() - start\n",
        "    print('Epoch [%d/%d], Loss: %.4f, Time (s): %d'\n",
        "          % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n",
        "        )\n",
        "    if epoch % 1 == 0:\n",
        "      torch.save(model.state_dict(), os.path.join(args.output_dir, 'memnet_epoch_{}.pth'.format(epoch)))\n",
        "\n",
        "  # Plot training curve\n",
        "  plt.figure()\n",
        "  plt.plot(train_losses, \"ro-\", label=\"Train\")\n",
        "  # plt.plot(valid_losses, \"go-\", label=\"Validation\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.show()\n",
        "  plt.savefig(args.output_dir + \"train_loss.png\")\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "dfRGHdI1PPQu",
        "outputId": "79d10a92-5b38-4626-893a-675fa70b95a9"
      },
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    'threads' : 1,\n",
        "    'kernel_size' : 3,\n",
        "    'N_long' : 6,\n",
        "    'N_short' : 6,\n",
        "    'd_conv' : 64,\n",
        "    'lr' : 0.05,\n",
        "    'momentum' : 0.9,\n",
        "    'w_decay' : 1E-4,\n",
        "    'clip' : 0.4,\n",
        "    'batch_size': 64,\n",
        "    'patch_size': 31,\n",
        "    'scale' : 2,\n",
        "    'epochs' : 40,\n",
        "    'train_dir' : directory + '/t91_crop_training/',\n",
        "    'output_dir' : drive_dir,\n",
        "    'start_from' : '/content/drive/MyDrive/memnet/memnet_ms_bsds/memnet_epoch_79.pth' # default is ''\n",
        "}\n",
        "args.update(args_dict)\n",
        "memnet = train(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting from \n",
            "Training starts...\n",
            "Epoch [1/40], Loss: 0.0296, Time (s): 53\n",
            "Epoch [2/40], Loss: 0.0285, Time (s): 106\n",
            "Epoch [3/40], Loss: 0.0280, Time (s): 158\n",
            "Epoch [4/40], Loss: 0.0275, Time (s): 211\n",
            "Epoch [5/40], Loss: 0.0274, Time (s): 264\n",
            "Epoch [6/40], Loss: 0.0269, Time (s): 317\n",
            "Epoch [7/40], Loss: 0.0264, Time (s): 370\n",
            "Epoch [8/40], Loss: 0.0261, Time (s): 423\n",
            "Epoch [9/40], Loss: 0.0260, Time (s): 476\n",
            "Epoch [10/40], Loss: 0.0258, Time (s): 529\n",
            "Epoch [11/40], Loss: 0.0241, Time (s): 582\n",
            "Epoch [12/40], Loss: 0.0233, Time (s): 635\n",
            "Epoch [13/40], Loss: 0.0229, Time (s): 688\n",
            "Epoch [14/40], Loss: 0.0226, Time (s): 741\n",
            "Epoch [15/40], Loss: 0.0223, Time (s): 794\n",
            "Epoch [16/40], Loss: 0.0220, Time (s): 847\n",
            "Epoch [17/40], Loss: 0.0218, Time (s): 900\n",
            "Epoch [18/40], Loss: 0.0216, Time (s): 953\n",
            "Epoch [19/40], Loss: 0.0214, Time (s): 1006\n",
            "Epoch [20/40], Loss: 0.0211, Time (s): 1059\n",
            "Epoch [21/40], Loss: 0.0207, Time (s): 1112\n",
            "Epoch [22/40], Loss: 0.0206, Time (s): 1165\n",
            "Epoch [23/40], Loss: 0.0205, Time (s): 1217\n",
            "Epoch [24/40], Loss: 0.0205, Time (s): 1270\n",
            "Epoch [25/40], Loss: 0.0204, Time (s): 1323\n",
            "Epoch [26/40], Loss: 0.0204, Time (s): 1376\n",
            "Epoch [27/40], Loss: 0.0204, Time (s): 1429\n",
            "Epoch [28/40], Loss: 0.0204, Time (s): 1482\n",
            "Epoch [29/40], Loss: 0.0203, Time (s): 1535\n",
            "Epoch [30/40], Loss: 0.0203, Time (s): 1588\n",
            "Epoch [31/40], Loss: 0.0203, Time (s): 1641\n",
            "Epoch [32/40], Loss: 0.0202, Time (s): 1694\n",
            "Epoch [33/40], Loss: 0.0202, Time (s): 1747\n",
            "Epoch [34/40], Loss: 0.0203, Time (s): 1800\n",
            "Epoch [35/40], Loss: 0.0202, Time (s): 1852\n",
            "Epoch [36/40], Loss: 0.0202, Time (s): 1905\n",
            "Epoch [37/40], Loss: 0.0202, Time (s): 1958\n",
            "Epoch [38/40], Loss: 0.0202, Time (s): 2011\n",
            "Epoch [39/40], Loss: 0.0202, Time (s): 2064\n",
            "Epoch [40/40], Loss: 0.0202, Time (s): 2117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmPzA7QDrR_k"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hD6dLXUlJdT"
      },
      "source": [
        "# args = AttrDict()\n",
        "# args_dict = {\n",
        "#     'threads' : 1,\n",
        "#     'kernel_size' : 3,\n",
        "#     'N_long' : 6,\n",
        "#     'N_short' : 6,\n",
        "#     'd_conv' : 64,\n",
        "#     'lr' : 0.1,\n",
        "#     'batch_size': 32,\n",
        "#     'patch_size': 41,\n",
        "#     'scale' : 2,\n",
        "#     'epochs' : 20,\n",
        "#     'train_dir' : directory + '/BSDS300/images/train/',\n",
        "#     'output_dir' : '/content/drive/MyDrive/memnet/'\n",
        "# }\n",
        "model = MemNet(args_dict['N_long'], args_dict['N_short'], args_dict['d_conv'])\n",
        "model.load_state_dict(torch.load(os.path.join(args_dict['output_dir'], 'memnet_epoch_{}.pth'.format(3)),\n",
        "                                 map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "model.requires_grad_ = False\n",
        "for param in model.parameters():\n",
        "    param.require_grads = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWufvhHXyOTF"
      },
      "source": [
        "crop_size = 200\n",
        "down_sample = 3\n",
        "files = sorted(glob.glob(directory + '/BSDS300/images/train/' + '/*'))\n",
        "index = random.randint(0,199)\n",
        "hr_image = PIL.Image.open(files[index])\n",
        "crop_bl = [100, 100]#[random.randint(0, hr_image.width - crop_size),\n",
        "           #random.randint(0, hr_image.height - crop_size)]\n",
        "hr_image = hr_image.crop((crop_bl[0], crop_bl[1],\n",
        "                          crop_bl[0]+crop_size, crop_bl[1]+crop_size))\n",
        "lr_width = hr_image.width // down_sample\n",
        "lr_height = hr_image.height // down_sample\n",
        "lr_image = hr_image.resize((lr_width, lr_height),\n",
        "                              resample=PIL.Image.BICUBIC, box=None, reducing_gap=None)\n",
        "lr_image = lr_image.resize((hr_image.width, hr_image.height),\n",
        "                              resample=PIL.Image.BICUBIC, box=None, reducing_gap=None)\n",
        "\n",
        "trsfm = transforms.Compose([transforms.ToTensor()])\n",
        "lr_ten = trsfm(lr_image)\n",
        "lr_ten = rgb_to_ycbcr(lr_ten)\n",
        "\n",
        "pred_c = model(lr_ten.unsqueeze(0)[:, 0, :, :].unsqueeze(1))\n",
        "# diff = pred_c - lr_ten.unsqueeze(0)[:, 0, :, :].unsqueeze(1)\n",
        "pred_c = pred_c.detach()\n",
        "pred_ten = lr_ten.detach().clone()\n",
        "pred_ten[0, :, :] = pred_c[0, 0, :, :]\n",
        "pred_ten = pred_ten.detach()\n",
        "\n",
        "pred_arr = ((255.0*ycbcr_to_rgb(pred_ten)).numpy().astype(np.uint8())).transpose([1, 2, 0])\n",
        "pred_image = PIL.Image.fromarray(pred_arr, 'RGB')\n",
        "display(hr_image)\n",
        "display(lr_image)\n",
        "display(pred_image)\n",
        "\n",
        "hr_ten = trsfm(hr_image)\n",
        "hr_ten = rgb_to_ycbcr(hr_ten)\n",
        "hr_bl_arr = ((255.0*hr_ten).numpy().astype(np.uint8())).transpose([1, 2, 0])\n",
        "lr_bl_arr = ((255.0*lr_ten).numpy().astype(np.uint8())).transpose([1, 2, 0])\n",
        "hr_bl_image = PIL.Image.fromarray(hr_bl_arr[:, :, 0], 'L')\n",
        "lr_bl_image = PIL.Image.fromarray(lr_bl_arr[:, :, 0], 'L')\n",
        "display(hr_bl_image)\n",
        "display(lr_bl_image)\n",
        "\n",
        "pred_bl_arr = (255.0*pred_c.squeeze()).numpy().astype(np.uint8())\n",
        "pred_bl_image = PIL.Image.fromarray(pred_bl_arr, 'L')\n",
        "display(pred_bl_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1822dtu3CV4"
      },
      "source": [
        "hr_image = PIL.Image.open('/content/drive/MyDrive/memnet/sample_tests/IMG-4035.JPG')\n",
        "en_width = int(1.5 * hr_image.width)\n",
        "en_height = int(1.5 * hr_image.height)\n",
        "en_image = hr_image.resize((en_width, en_height),\n",
        "                              resample=PIL.Image.BICUBIC, box=None, reducing_gap=None)\n",
        "\n",
        "trsfm = transforms.Compose([transforms.ToTensor()])\n",
        "en_ten = trsfm(en_image)\n",
        "en_ten = rgb_to_ycbcr(en_ten)\n",
        "\n",
        "pred_c = model(en_ten.unsqueeze(0)[:, 0, :, :].unsqueeze(1))\n",
        "pred_c = pred_c.detach()\n",
        "pred_ten = en_ten.detach().clone()\n",
        "pred_ten[0, :, :] = pred_c[0, 0, :, :]\n",
        "pred_ten = pred_ten.detach()\n",
        "\n",
        "pred_arr_cv = cv2.normalize((ycbcr_to_rgb(pred_ten).squeeze()).numpy().transpose([1, 2, 0]),\n",
        "                            None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "pred_arr = ((255.0*ycbcr_to_rgb(pred_ten)).numpy().astype(np.uint8())).transpose([1, 2, 0])\n",
        "pred_image = PIL.Image.fromarray(pred_arr, 'RGB')\n",
        "pred_image_cv = PIL.Image.fromarray(pred_arr_cv, 'RGB')\n",
        "display(hr_image.transpose(PIL.Image.ROTATE_270))\n",
        "display(en_image.transpose(PIL.Image.ROTATE_270))\n",
        "display(pred_image.transpose(PIL.Image.ROTATE_270))\n",
        "display(pred_image_cv.transpose(PIL.Image.ROTATE_270))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}