{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"memnet_ss_3s3m.ipynb","provenance":[],"collapsed_sections":["KuMYUztgp6h_","uO1JLjItq4Pd"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyOGorKGyP9XwyGLH5HcSuNl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DS9_H89C0o1b"},"source":["## Module setup"]},{"cell_type":"markdown","metadata":{"id":"4a0uXkmyFWMe"},"source":["# Changes: \n","1) Training set: BSDS200, T91, General100\n","\n","2) training patch 31x31 and stride is 21."]},{"cell_type":"code","metadata":{"id":"khmWyAd17fbU","executionInfo":{"status":"ok","timestamp":1618945953017,"user_tz":240,"elapsed":3347,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["import os\n","import tarfile\n","import glob\n","import io\n","import random\n","from tqdm import tqdm\n","import shutil\n","import tarfile\n","import PIL\n","from IPython.display import display, Image\n","import numpy as np\n","from six.moves.urllib.request import urlretrieve\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision import transforms"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KuMYUztgp6h_"},"source":["## Helper \n","# RGB -> YCbCr and YCbCr -> RGB image conversion\n","This part is taken from Kornia\n","https://kornia.readthedocs.io/en/latest/_modules/kornia/color/ycbcr.html"]},{"cell_type":"code","metadata":{"id":"ZJlq-WlfqQ69","executionInfo":{"status":"ok","timestamp":1618945953431,"user_tz":240,"elapsed":406,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["def rgb_to_ycbcr(image: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Convert an RGB image to YCbCr.\n","\n","    Args:\n","        image (torch.Tensor): RGB Image to be converted to YCbCr with shape :math:`(*, 3, H, W)`.\n","\n","    Returns:\n","        torch.Tensor: YCbCr version of the image with shape :math:`(*, 3, H, W)`.\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> output = rgb_to_ycbcr(input)  # 2x3x4x5\n","    \"\"\"\n","    if not isinstance(image, torch.Tensor):\n","        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n","            type(image)))\n","\n","    if len(image.shape) < 3 or image.shape[-3] != 3:\n","        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n","                         .format(image.shape))\n","\n","    r: torch.Tensor = image[..., 0, :, :]\n","    g: torch.Tensor = image[..., 1, :, :]\n","    b: torch.Tensor = image[..., 2, :, :]\n","\n","    delta: float = 0.5\n","    y: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n","    cb: torch.Tensor = (b - y) * 0.564 + delta\n","    cr: torch.Tensor = (r - y) * 0.713 + delta\n","    return torch.stack([y, cb, cr], -3)\n","\n","\n","\n","def ycbcr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Convert an YCbCr image to RGB.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Args:\n","        image (torch.Tensor): YCbCr Image to be converted to RGB with shape :math:`(*, 3, H, W)`.\n","\n","    Returns:\n","        torch.Tensor: RGB version of the image with shape :math:`(*, 3, H, W)`.\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> output = ycbcr_to_rgb(input)  # 2x3x4x5\n","    \"\"\"\n","    if not isinstance(image, torch.Tensor):\n","        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n","            type(image)))\n","\n","    if len(image.shape) < 3 or image.shape[-3] != 3:\n","        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n","                         .format(image.shape))\n","\n","    y: torch.Tensor = image[..., 0, :, :]\n","    cb: torch.Tensor = image[..., 1, :, :]\n","    cr: torch.Tensor = image[..., 2, :, :]\n","\n","    delta: float = 0.5\n","    cb_shifted: torch.Tensor = cb - delta\n","    cr_shifted: torch.Tensor = cr - delta\n","\n","    r: torch.Tensor = y + 1.403 * cr_shifted\n","    g: torch.Tensor = y - 0.714 * cr_shifted - 0.344 * cb_shifted\n","    b: torch.Tensor = y + 1.773 * cb_shifted\n","    return torch.stack([r, g, b], -3)\n","\n","\n","\n","class RgbToYcbcr(nn.Module):\n","    \"\"\"Convert an image from RGB to YCbCr.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Returns:\n","        torch.Tensor: YCbCr version of the image.\n","\n","    Shape:\n","        - image: :math:`(*, 3, H, W)`\n","        - output: :math:`(*, 3, H, W)`\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> ycbcr = RgbToYcbcr()\n","        >>> output = ycbcr(input)  # 2x3x4x5\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(RgbToYcbcr, self).__init__()\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        return rgb_to_ycbcr(image)\n","\n","\n","\n","class YcbcrToRgb(nn.Module):\n","    \"\"\"Convert an image from YCbCr to Rgb.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Returns:\n","        torch.Tensor: RGB version of the image.\n","\n","    Shape:\n","        - image: :math:`(*, 3, H, W)`\n","        - output: :math:`(*, 3, H, W)`\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> rgb = YcbcrToRgb()\n","        >>> output = rgb(input)  # 2x3x4x5\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(YcbcrToRgb, self).__init__()\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        return ycbcr_to_rgb(image)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eO181ezuqXK6"},"source":["## Directory setup and preparing the datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6cRBsi_0Z2s","executionInfo":{"status":"ok","timestamp":1618945970576,"user_tz":240,"elapsed":15486,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"5542358b-01c8-442b-9787-c5f20966c386"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z9bnrCCa4Y_E","executionInfo":{"status":"ok","timestamp":1618945993772,"user_tz":240,"elapsed":205,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["directory = '/content/memnet'\n","if not os.path.exists(directory):\n","  os.makedirs(directory)\n","\n","os.chdir(directory)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uO1JLjItq4Pd"},"source":["## Model : Single-Supervised MemNet "]},{"cell_type":"code","metadata":{"id":"A5rUr_6uDYZg","executionInfo":{"status":"ok","timestamp":1618945996341,"user_tz":240,"elapsed":332,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class FeatExtBlock(nn.Module):\n","    def __init__(self, in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1):\n","        super(FeatExtBlock, self).__init__()\n","        self.feature = nn.Sequential(nn.BatchNorm2d(1), nn.ReLU(),\n","                                     nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n","                                               kernel_size=kernel_size, stride=stride,\n","                                               padding=padding, bias=False))\n","\n","    def forward(self, x):\n","        fe_out = self.feature(x)\n","        return fe_out"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoywv9Bk5HNp","executionInfo":{"status":"ok","timestamp":1618945996342,"user_tz":240,"elapsed":324,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class ResidualBlock(nn.Module):\n","  def __init__(self, n_layers=2, n_channels=64, kernel_size=3, stride=1, padding=1):\n","    super(ResidualBlock, self).__init__()\n","    layers = []\n","    for _ in range(n_layers):\n","      layers.append(nn.BatchNorm2d(n_channels))\n","      layers.append(nn.ReLU())\n","      layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n","                              stride=stride, padding=padding, bias=False))\n","    \n","    self.residual = nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    out = self.residual(x)\n","    return out + x"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"awmHxvKb69Xl","executionInfo":{"status":"ok","timestamp":1618945996342,"user_tz":240,"elapsed":316,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class RecursiveUnit(nn.Module):\n","  def __init__(self, n_short=6, d_conv=64):\n","    super(RecursiveUnit, self).__init__()\n","    self.n_short = n_short\n","    self.residual = nn.ModuleList([ResidualBlock(n_layers=2, n_channels=d_conv) for _ in range(n_short)])\n","\n","  def forward(self, b_prev_m):\n","    Hs = []\n","    Hs.append(self.residual[0](b_prev_m))\n","    for rec in range(1, self.n_short):\n","      Hs.append(self.residual[rec](Hs[-1]))\n","    b_short = torch.cat(Hs, dim=1)\n","    return b_short"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CqUd6dgpRBgA","executionInfo":{"status":"ok","timestamp":1618945996343,"user_tz":240,"elapsed":309,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class GateUnit(nn.Module):\n","    def __init__(self, in_channels, d_conv=64):\n","        super(GateUnit, self).__init__()\n","        self.in_channels = in_channels\n","        self.gate_layer = nn.Sequential(nn.BatchNorm2d(in_channels), nn.ReLU(),\n","                                  nn.Conv2d(in_channels=self.in_channels, out_channels=d_conv, kernel_size=1,\n","                                            stride=1, padding=0, bias=False))\n","\n","    def forward(self, b_gate):\n","        b_mem = self.gate_layer(b_gate)\n","        return b_mem"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8MftIwz-CIk","executionInfo":{"status":"ok","timestamp":1618945996343,"user_tz":240,"elapsed":301,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class MemBlock(nn.Module):\n","  def __init__(self, n_long, N_short=6, d_conv=64):\n","    super(MemBlock, self).__init__()\n","    self.n_long = n_long\n","    self.N_long = N_short\n","    self.d_conv = d_conv\n","    self.gate_in = (n_long + N_short) * d_conv\n","    self.short_unit = RecursiveUnit(N_short, d_conv)\n","    self.gate = GateUnit(self.gate_in, self.d_conv)\n","\n","  def forward(self, b_long):\n","    b_short = self.short_unit(b_long[:, -self.d_conv:, :, :])\n","    b_gate = torch.cat([b_short, b_long], dim=1)\n","    b_m = self.gate(b_gate)\n","    return b_m"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZVQ3ZRZFLNj","executionInfo":{"status":"ok","timestamp":1618945996344,"user_tz":240,"elapsed":294,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class ReconstBlock(nn.Module):\n","  def __init__(self, d_conv=64, kernel_size=3, stride=1, padding=1):\n","    super(ReconstBlock, self).__init__()\n","    self.d_conv = d_conv\n","    self.recon = nn.Sequential(nn.BatchNorm2d(d_conv), nn.ReLU(),\n","                               nn.Conv2d(in_channels=d_conv, out_channels=1, kernel_size=kernel_size,\n","                                         stride=stride, padding=padding, bias=False))\n","\n","  def forward(self, b_m, x):\n","    y = self.recon(b_m)\n","    return y+x"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEyTxqG0xfAK","executionInfo":{"status":"ok","timestamp":1618945996344,"user_tz":240,"elapsed":286,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class MemNet_SS(nn.Module):\n","    def __init__(self, N_long, N_short, d_conv=64):\n","        super(MemNet_SS, self).__init__()\n","        self.N_long = N_long\n","        self.N_short = N_short\n","        self.d_conv = d_conv\n","        self.fe_block = FeatExtBlock(1, d_conv)\n","        self.mem_blocks = nn.ModuleList([MemBlock(n_mem+1, N_short, d_conv) for n_mem in range(N_long)])\n","        self.recon_block = ReconstBlock(d_conv)\n","        # This part is taken from: https://discuss.pytorch.org/t/contraining-weights-to-sum-to-1/20609/2\n","        # self.eps = 1E-7\n","        # self.linavg = nn.Parameter((1./self.N_long) * torch.ones(N_long), requires_grad=True)\n","\n","    def forward(self, x):\n","      fe = self.fe_block(x)\n","      b_ms = []\n","      b_ms.append(fe)\n","      y = torch.zeros_like(x)\n","      for n_mem in range(self.N_long):\n","        b_long = torch.cat(b_ms, dim=1)\n","        b_ms.append(self.mem_blocks[n_mem](b_long))\n","        # self.linavg = self.linavg.clamp(min=self.eps) # in case weights > 0\n","        # linavg_sum = self.linavg.sum(0, keepdim=True)\n","        # y += self.linavg[n_mem] * self.recon_block(b_ms[-1], x) / linavg_sum\n","      \n","      y = self.recon_block(b_ms[-1], x)\n","      return y"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sszPhXlL1B6b","executionInfo":{"status":"ok","timestamp":1618945996874,"user_tz":240,"elapsed":808,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"0ecbb340-a574-4129-b0e4-71c158e7f0b0"},"source":["def size_check():\n","  p = torch.rand(2, 1, 4, 4)\n","  print(p.size())\n","  d_conv = 8\n","  fe_model = FeatExtBlock(1, d_conv)\n","  q = fe_model(p)\n","  print(q.size())\n","  mem_model = MemBlock(1, 6, d_conv)\n","  r = mem_model(q)\n","  print(r.size())\n","  recon_model = ReconstBlock(d_conv)\n","  s = recon_model(r, p)\n","  print(s.size())\n","  model = MemNet_SS(2, 2, 32)\n","  t = model(p)\n","  print(t.size())\n","\n","size_check()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["torch.Size([2, 1, 4, 4])\n","torch.Size([2, 8, 4, 4])\n","torch.Size([2, 8, 4, 4])\n","torch.Size([2, 1, 4, 4])\n","torch.Size([2, 1, 4, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zkrs29MLquvQ"},"source":["## Dataset Preparation"]},{"cell_type":"code","metadata":{"id":"6R5hB9SzAzpi","executionInfo":{"status":"ok","timestamp":1618945998819,"user_tz":240,"elapsed":345,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class DatasetSR(Dataset):\n","  def __init__(self, images_dir, transform=None):\n","    self.files = []\n","    for dir in images_dir:\n","      self.files += glob.glob(dir + '/*')\n","    self.files = sorted(self.files)\n","    self.transform=transform\n","  \n","  def __len__(self):\n","    return len(self.files)\n","\n","  def __getitem__(self, index):\n","    image_path = self.files[index]\n","    hr_image = PIL.Image.open(image_path)\n","    hr_image = hr_image.convert('YCbCr')\n","    # random_gen = int(image_path.split('/')[-1].split('_')[-2][-1]) + int(image_path.split('/')[-1].split('_')[-1][-5])\n","    # scale = random_gen % 3 + 2 \n","    scale = random.randint(2, 4)\n","    width_down = hr_image.width // scale\n","    heigth_down = hr_image.height // scale\n","    lr_image = hr_image.resize((width_down, heigth_down),\n","                               resample=PIL.Image.BICUBIC)\n","    lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                               resample=PIL.Image.BICUBIC)\n","    \n","    trans_toten = transforms.ToTensor()\n","    hr_ten = trans_toten(hr_image)\n","    lr_ten = trans_toten(lr_image)\n","    hr_ten = rgb_to_ycbcr(hr_ten)\n","    lr_ten = rgb_to_ycbcr(lr_ten)\n","    \n","    if self.transform:\n","            hr = self.transform(hr_image)\n","            lr = self.transform(lr_image)\n","\n","    return hr, lr"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_mQTJeIrHfl"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"2r1QT0atLrnY","executionInfo":{"status":"ok","timestamp":1618946001145,"user_tz":240,"elapsed":625,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class AttrDict(dict):\n","  def __init__(self, *args, **kwargs):\n","    super(AttrDict, self).__init__(*args, **kwargs)\n","    self.__dict__ = self\n","\n","def weights_init(m):\n","  if isinstance(m, nn.Conv2d):\n","    nn.init.xavier_normal_(m.weight.data, gain=1.0)\n","    # nn.init.zeros_(m.bias.data)\n","\n","# The following function si taken from: https://medium.com/analytics-vidhya/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61\n","def load_ckp(checkpoint_fpath, model):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    model.load_state_dict(checkpoint)\n","    # optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model\n","\n","def train(args):\n","  model = MemNet_SS(args.N_long, args.N_short, args.d_conv)\n","  model.train()\n","  if torch.cuda.is_available():\n","    print(\"sending model to cuda...\")\n","    model = model.cuda()\n","  # optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","  optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.w_decay)\n","  if args['start_from']:\n","    print('starting from {}'.format(args.start_from))\n","    checkpoint = torch.load(args.start_from)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    # model = load_ckp(args['start_from'], model)\n","  else:\n","    print(\"initializing the model...\")\n","    model.apply(weights_init)\n","  \n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.epochs//8, gamma=args.gamma)\n","  criterion = nn.MSELoss(reduction='sum')\n","  trnsfrm = transforms.Compose([transforms.ToTensor()])\n","  # dataset = Dataset(opt.images_dir, opt.patch_size, opt.jpeg_quality, opt.use_fast_loader)\n","  dataset = DatasetSR(images_dir=args.train_dir, transform=trnsfrm)\n","  dataloader = DataLoader(dataset=dataset,\n","                          batch_size=args.batch_size,\n","                          shuffle=True,\n","                          num_workers=args.threads,\n","                          pin_memory=True,\n","                          drop_last=False)\n","  \n","  print('Training starts...')\n","  start = time.time()\n","  train_losses = []\n","  valid_losses = []\n","  for epoch in range(args.epochs):\n","    losses = []\n","    for i, data in enumerate(dataloader):\n","      y, x = data\n","      x = x[:, 0, :, :].unsqueeze(1)\n","      y = y[:, 0, :, :].unsqueeze(1)\n","      if torch.cuda.is_available():\n","        x = x.cuda()\n","        y = y.cuda()\n","      \n","      x_hat = model(x)\n","      # print(\"evaluated\")\n","      loss = (0.5/args.batch_size)*criterion(x_hat, y)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      nn.utils.clip_grad_norm_(model.parameters(), args.clip) \n","      optimizer.step()\n","      losses.append(loss.data.item())\n","\n","    # print(\"moving forward...\")\n","    scheduler.step()\n","    avg_loss = np.mean(losses)\n","    train_losses.append(avg_loss)\n","    time_elapsed = time.time() - start\n","    print('Epoch [%d/%d], Loss: %.4f, Time (s): %d'\n","          % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n","        )\n","    if (epoch+1) % 5 == 0:\n","      patch_checkpoint = os.path.join(args.output_dir, 'memnet_epoch_{}.pt'.format(epoch+1))\n","      torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, patch_checkpoint)\n","      # torch.save(model.state_dict(), os.path.join(args.output_dir, 'memnet_epoch_{}.pt'.format(epoch+1)))\n","\n","  # Plot training curve\n","  plt.figure()\n","  plt.plot(train_losses, \"ro-\", label=\"Train\")\n","  # plt.plot(valid_losses, \"go-\", label=\"Validation\")\n","  plt.legend()\n","  plt.title(\"Loss\")\n","  plt.xlabel(\"Epochs\")\n","  plt.show()\n","  plt.savefig(os.path.join(args.output_dir, \"train_loss.png\"))\n","  \n","  return model"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfRGHdI1PPQu","executionInfo":{"status":"ok","timestamp":1618958655182,"user_tz":240,"elapsed":299,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["drive_dir = '/content/drive/MyDrive/memnet/memnet_ss_3s3m'\n","if not os.path.exists(drive_dir):\n","  os.makedirs(drive_dir)\n","\n","patch_size = 31\n","trainings = ['BSDS200_p'+str(patch_size), 'T91_p'+str(patch_size), 'General100_p'+str(patch_size)]\n","args = AttrDict()\n","args_dict = {\n","    'threads' : 1,\n","    'kernel_size' : 3,\n","    'N_long' : 3,\n","    'N_short' : 3,\n","    'd_conv' : 64,\n","    'lr' : 0.1,\n","    'momentum' : 0.9,\n","    'w_decay' : 1E-4,\n","    'clip' : 0.4,\n","    'gamma' : 0.2,\n","    'batch_size':128,\n","    'patch_size': patch_size,\n","    'epochs' : 80,\n","    'train_dir' : [directory+'/'+trainings[i] for i in range(len(trainings))],\n","    'output_dir' : drive_dir,\n","    'start_from' : ''\n","}\n","args.update(args_dict)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCGLKe5dTbNk","executionInfo":{"status":"ok","timestamp":1618946049759,"user_tz":240,"elapsed":39926,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"4d8d6a95-bae3-4eb7-d29b-5946f2a938d5"},"source":["gdrive_tdir = '/content/drive/MyDrive/memnet'\n","for tri in trainings:\n","  shutil.copy(os.path.join(gdrive_tdir, tri + '.tar.gz'), directory)\n","  file = tarfile.open(os.path.join(directory, tri + '.tar.gz'))\n","  file.extractall('./')\n","  file.close()\n","  print(len(next(os.walk('./'+tri))[2]))\n","\n","# !ls /content/drive/MyDrive/memnet\n","# !cp /content/drive/MyDrive/memnet/T91_p51.tar.gz /content/memnet/T91_p51.tar.gz\n","# with tarfile.open('T91_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !cp /content/drive/MyDrive/memnet/BSDS200_p51.tar.gz /content/memnet/BSDS200_p51.tar.gz\n","# with tarfile.open('BSDS200_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !cp /content/drive/MyDrive/memnet/General100_p51.tar.gz /content/memnet/General100_p51.tar.gz\n","# with tarfile.open('General100_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !ls /content/memnet/T91_p51/ -1 | wc -l\n","# !ls /content/memnet/BSDS200_p51/ -1 | wc -l\n","# !ls /content/memnet/General100_p31/ -1 | wc -l"],"execution_count":16,"outputs":[{"output_type":"stream","text":["73600\n","13115\n","42803\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uoGJUelbTfGc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1618957717856,"user_tz":240,"elapsed":11656348,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"0b0bbf93-e829-4033-c156-ce8cdbabd92d"},"source":["memnet = train(args)\n","while True:pass"],"execution_count":17,"outputs":[{"output_type":"stream","text":["sending model to cuda...\n","initializing the model...\n","Training starts...\n","Epoch [1/80], Loss: 4.7512, Time (s): 137\n","Epoch [2/80], Loss: 1.1722, Time (s): 275\n","Epoch [3/80], Loss: 1.1388, Time (s): 413\n","Epoch [4/80], Loss: 1.1196, Time (s): 550\n","Epoch [5/80], Loss: 1.1125, Time (s): 688\n","Epoch [6/80], Loss: 1.1052, Time (s): 826\n","Epoch [7/80], Loss: 1.1005, Time (s): 963\n","Epoch [8/80], Loss: 1.0922, Time (s): 1101\n","Epoch [9/80], Loss: 1.0922, Time (s): 1239\n","Epoch [10/80], Loss: 1.0851, Time (s): 1377\n","Epoch [11/80], Loss: 1.0583, Time (s): 1514\n","Epoch [12/80], Loss: 1.0521, Time (s): 1652\n","Epoch [13/80], Loss: 1.0493, Time (s): 1789\n","Epoch [14/80], Loss: 1.0467, Time (s): 1927\n","Epoch [15/80], Loss: 1.0487, Time (s): 2065\n","Epoch [16/80], Loss: 1.0495, Time (s): 2203\n","Epoch [17/80], Loss: 1.0407, Time (s): 2340\n","Epoch [18/80], Loss: 1.0441, Time (s): 2478\n","Epoch [19/80], Loss: 1.0450, Time (s): 2616\n","Epoch [20/80], Loss: 1.0418, Time (s): 2753\n","Epoch [21/80], Loss: 1.0291, Time (s): 2891\n","Epoch [22/80], Loss: 1.0272, Time (s): 3029\n","Epoch [23/80], Loss: 1.0255, Time (s): 3167\n","Epoch [24/80], Loss: 1.0305, Time (s): 3304\n","Epoch [25/80], Loss: 1.0274, Time (s): 3442\n","Epoch [26/80], Loss: 1.0234, Time (s): 3580\n","Epoch [27/80], Loss: 1.0271, Time (s): 3718\n","Epoch [28/80], Loss: 1.0218, Time (s): 3855\n","Epoch [29/80], Loss: 1.0196, Time (s): 3993\n","Epoch [30/80], Loss: 1.0268, Time (s): 4131\n","Epoch [31/80], Loss: 1.0214, Time (s): 4268\n","Epoch [32/80], Loss: 1.0238, Time (s): 4406\n","Epoch [33/80], Loss: 1.0172, Time (s): 4544\n","Epoch [34/80], Loss: 1.0227, Time (s): 4682\n","Epoch [35/80], Loss: 1.0203, Time (s): 4819\n","Epoch [36/80], Loss: 1.0210, Time (s): 4957\n","Epoch [37/80], Loss: 1.0178, Time (s): 5095\n","Epoch [38/80], Loss: 1.0250, Time (s): 5232\n","Epoch [39/80], Loss: 1.0188, Time (s): 5370\n","Epoch [40/80], Loss: 1.0167, Time (s): 5508\n","Epoch [41/80], Loss: 1.0194, Time (s): 5646\n","Epoch [42/80], Loss: 1.0202, Time (s): 5783\n","Epoch [43/80], Loss: 1.0174, Time (s): 5921\n","Epoch [44/80], Loss: 1.0231, Time (s): 6058\n","Epoch [45/80], Loss: 1.0210, Time (s): 6196\n","Epoch [46/80], Loss: 1.0168, Time (s): 6334\n","Epoch [47/80], Loss: 1.0227, Time (s): 6471\n","Epoch [48/80], Loss: 1.0214, Time (s): 6609\n","Epoch [49/80], Loss: 1.0184, Time (s): 6746\n","Epoch [50/80], Loss: 1.0193, Time (s): 6884\n","Epoch [51/80], Loss: 1.0218, Time (s): 7022\n","Epoch [52/80], Loss: 1.0182, Time (s): 7159\n","Epoch [53/80], Loss: 1.0210, Time (s): 7296\n","Epoch [54/80], Loss: 1.0152, Time (s): 7434\n","Epoch [55/80], Loss: 1.0159, Time (s): 7571\n","Epoch [56/80], Loss: 1.0180, Time (s): 7709\n","Epoch [57/80], Loss: 1.0186, Time (s): 7846\n","Epoch [58/80], Loss: 1.0177, Time (s): 7984\n","Epoch [59/80], Loss: 1.0165, Time (s): 8121\n","Epoch [60/80], Loss: 1.0222, Time (s): 8259\n","Epoch [61/80], Loss: 1.0187, Time (s): 8396\n","Epoch [62/80], Loss: 1.0184, Time (s): 8534\n","Epoch [63/80], Loss: 1.0160, Time (s): 8672\n","Epoch [64/80], Loss: 1.0193, Time (s): 8809\n","Epoch [65/80], Loss: 1.0182, Time (s): 8947\n","Epoch [66/80], Loss: 1.0171, Time (s): 9085\n","Epoch [67/80], Loss: 1.0208, Time (s): 9222\n","Epoch [68/80], Loss: 1.0188, Time (s): 9360\n","Epoch [69/80], Loss: 1.0211, Time (s): 9498\n","Epoch [70/80], Loss: 1.0257, Time (s): 9635\n","Epoch [71/80], Loss: 1.0176, Time (s): 9773\n","Epoch [72/80], Loss: 1.0189, Time (s): 9911\n","Epoch [73/80], Loss: 1.0201, Time (s): 10049\n","Epoch [74/80], Loss: 1.0205, Time (s): 10186\n","Epoch [75/80], Loss: 1.0187, Time (s): 10324\n","Epoch [76/80], Loss: 1.0123, Time (s): 10463\n","Epoch [77/80], Loss: 1.0200, Time (s): 10600\n","Epoch [78/80], Loss: 1.0149, Time (s): 10738\n","Epoch [79/80], Loss: 1.0210, Time (s): 10876\n","Epoch [80/80], Loss: 1.0173, Time (s): 11013\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b289ffe118e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmemnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"tWH1JKdkdMlt","executionInfo":{"status":"ok","timestamp":1618958673652,"user_tz":240,"elapsed":628,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["patch_checkpoint = os.path.join(args.output_dir, 'memnet_epoch_{}.pt'.format(80))\n","torch.save({\n","      'epoch': 80,\n","      'model_state_dict': memnet.state_dict(),\n","      'optimizer_state_dict': None,\n","      'loss': None,\n","      }, patch_checkpoint)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkXQ4h1ZdORA"},"source":[""],"execution_count":null,"outputs":[]}]}