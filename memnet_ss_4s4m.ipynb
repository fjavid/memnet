{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"memnet_ss_4s4m.ipynb","provenance":[],"collapsed_sections":["KuMYUztgp6h_","uO1JLjItq4Pd"],"machine_shape":"hm","authorship_tag":"ABX9TyPAN3s3mJZbOQIMBySStQ1C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DS9_H89C0o1b"},"source":["## Module setup"]},{"cell_type":"markdown","metadata":{"id":"4a0uXkmyFWMe"},"source":["# Changes: \n","1) Training set: BSDS200, T91, General100\n","\n","2) training patch 31x31 and stride is 21."]},{"cell_type":"code","metadata":{"id":"khmWyAd17fbU","executionInfo":{"status":"ok","timestamp":1618945850499,"user_tz":240,"elapsed":2935,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["import os\n","import tarfile\n","import glob\n","import io\n","import random\n","from tqdm import tqdm\n","import shutil\n","import tarfile\n","import PIL\n","from IPython.display import display, Image\n","import numpy as np\n","from six.moves.urllib.request import urlretrieve\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision import transforms"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KuMYUztgp6h_"},"source":["## Helper \n","# RGB -> YCbCr and YCbCr -> RGB image conversion\n","This part is taken from Kornia\n","https://kornia.readthedocs.io/en/latest/_modules/kornia/color/ycbcr.html"]},{"cell_type":"code","metadata":{"id":"ZJlq-WlfqQ69","executionInfo":{"status":"ok","timestamp":1618945851633,"user_tz":240,"elapsed":1128,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["def rgb_to_ycbcr(image: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Convert an RGB image to YCbCr.\n","\n","    Args:\n","        image (torch.Tensor): RGB Image to be converted to YCbCr with shape :math:`(*, 3, H, W)`.\n","\n","    Returns:\n","        torch.Tensor: YCbCr version of the image with shape :math:`(*, 3, H, W)`.\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> output = rgb_to_ycbcr(input)  # 2x3x4x5\n","    \"\"\"\n","    if not isinstance(image, torch.Tensor):\n","        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n","            type(image)))\n","\n","    if len(image.shape) < 3 or image.shape[-3] != 3:\n","        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n","                         .format(image.shape))\n","\n","    r: torch.Tensor = image[..., 0, :, :]\n","    g: torch.Tensor = image[..., 1, :, :]\n","    b: torch.Tensor = image[..., 2, :, :]\n","\n","    delta: float = 0.5\n","    y: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n","    cb: torch.Tensor = (b - y) * 0.564 + delta\n","    cr: torch.Tensor = (r - y) * 0.713 + delta\n","    return torch.stack([y, cb, cr], -3)\n","\n","\n","\n","def ycbcr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Convert an YCbCr image to RGB.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Args:\n","        image (torch.Tensor): YCbCr Image to be converted to RGB with shape :math:`(*, 3, H, W)`.\n","\n","    Returns:\n","        torch.Tensor: RGB version of the image with shape :math:`(*, 3, H, W)`.\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> output = ycbcr_to_rgb(input)  # 2x3x4x5\n","    \"\"\"\n","    if not isinstance(image, torch.Tensor):\n","        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n","            type(image)))\n","\n","    if len(image.shape) < 3 or image.shape[-3] != 3:\n","        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n","                         .format(image.shape))\n","\n","    y: torch.Tensor = image[..., 0, :, :]\n","    cb: torch.Tensor = image[..., 1, :, :]\n","    cr: torch.Tensor = image[..., 2, :, :]\n","\n","    delta: float = 0.5\n","    cb_shifted: torch.Tensor = cb - delta\n","    cr_shifted: torch.Tensor = cr - delta\n","\n","    r: torch.Tensor = y + 1.403 * cr_shifted\n","    g: torch.Tensor = y - 0.714 * cr_shifted - 0.344 * cb_shifted\n","    b: torch.Tensor = y + 1.773 * cb_shifted\n","    return torch.stack([r, g, b], -3)\n","\n","\n","\n","class RgbToYcbcr(nn.Module):\n","    \"\"\"Convert an image from RGB to YCbCr.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Returns:\n","        torch.Tensor: YCbCr version of the image.\n","\n","    Shape:\n","        - image: :math:`(*, 3, H, W)`\n","        - output: :math:`(*, 3, H, W)`\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> ycbcr = RgbToYcbcr()\n","        >>> output = ycbcr(input)  # 2x3x4x5\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(RgbToYcbcr, self).__init__()\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        return rgb_to_ycbcr(image)\n","\n","\n","\n","class YcbcrToRgb(nn.Module):\n","    \"\"\"Convert an image from YCbCr to Rgb.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Returns:\n","        torch.Tensor: RGB version of the image.\n","\n","    Shape:\n","        - image: :math:`(*, 3, H, W)`\n","        - output: :math:`(*, 3, H, W)`\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> rgb = YcbcrToRgb()\n","        >>> output = rgb(input)  # 2x3x4x5\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(YcbcrToRgb, self).__init__()\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        return ycbcr_to_rgb(image)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eO181ezuqXK6"},"source":["## Directory setup and preparing the datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6cRBsi_0Z2s","executionInfo":{"status":"ok","timestamp":1618945872182,"user_tz":240,"elapsed":20849,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"a2de29c5-b247-4167-ada9-f6af482d4dbe"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z9bnrCCa4Y_E","executionInfo":{"status":"ok","timestamp":1618945875010,"user_tz":240,"elapsed":455,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["directory = '/content/memnet'\n","if not os.path.exists(directory):\n","  os.makedirs(directory)\n","\n","os.chdir(directory)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uO1JLjItq4Pd"},"source":["## Model : Single-Supervised MemNet "]},{"cell_type":"code","metadata":{"id":"A5rUr_6uDYZg","executionInfo":{"status":"ok","timestamp":1618945879267,"user_tz":240,"elapsed":607,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class FeatExtBlock(nn.Module):\n","    def __init__(self, in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1):\n","        super(FeatExtBlock, self).__init__()\n","        self.feature = nn.Sequential(nn.BatchNorm2d(1), nn.ReLU(),\n","                                     nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n","                                               kernel_size=kernel_size, stride=stride,\n","                                               padding=padding, bias=False))\n","\n","    def forward(self, x):\n","        fe_out = self.feature(x)\n","        return fe_out"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoywv9Bk5HNp","executionInfo":{"status":"ok","timestamp":1618945879548,"user_tz":240,"elapsed":879,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class ResidualBlock(nn.Module):\n","  def __init__(self, n_layers=2, n_channels=64, kernel_size=3, stride=1, padding=1):\n","    super(ResidualBlock, self).__init__()\n","    layers = []\n","    for _ in range(n_layers):\n","      layers.append(nn.BatchNorm2d(n_channels))\n","      layers.append(nn.ReLU())\n","      layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n","                              stride=stride, padding=padding, bias=False))\n","    \n","    self.residual = nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    out = self.residual(x)\n","    return out + x"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"awmHxvKb69Xl","executionInfo":{"status":"ok","timestamp":1618945879549,"user_tz":240,"elapsed":871,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class RecursiveUnit(nn.Module):\n","  def __init__(self, n_short=6, d_conv=64):\n","    super(RecursiveUnit, self).__init__()\n","    self.n_short = n_short\n","    self.residual = nn.ModuleList([ResidualBlock(n_layers=2, n_channels=d_conv) for _ in range(n_short)])\n","\n","  def forward(self, b_prev_m):\n","    Hs = []\n","    Hs.append(self.residual[0](b_prev_m))\n","    for rec in range(1, self.n_short):\n","      Hs.append(self.residual[rec](Hs[-1]))\n","    b_short = torch.cat(Hs, dim=1)\n","    return b_short"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CqUd6dgpRBgA","executionInfo":{"status":"ok","timestamp":1618945879549,"user_tz":240,"elapsed":863,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class GateUnit(nn.Module):\n","    def __init__(self, in_channels, d_conv=64):\n","        super(GateUnit, self).__init__()\n","        self.in_channels = in_channels\n","        self.gate_layer = nn.Sequential(nn.BatchNorm2d(in_channels), nn.ReLU(),\n","                                  nn.Conv2d(in_channels=self.in_channels, out_channels=d_conv, kernel_size=1,\n","                                            stride=1, padding=0, bias=False))\n","\n","    def forward(self, b_gate):\n","        b_mem = self.gate_layer(b_gate)\n","        return b_mem"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8MftIwz-CIk","executionInfo":{"status":"ok","timestamp":1618945879550,"user_tz":240,"elapsed":856,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class MemBlock(nn.Module):\n","  def __init__(self, n_long, N_short=6, d_conv=64):\n","    super(MemBlock, self).__init__()\n","    self.n_long = n_long\n","    self.N_long = N_short\n","    self.d_conv = d_conv\n","    self.gate_in = (n_long + N_short) * d_conv\n","    self.short_unit = RecursiveUnit(N_short, d_conv)\n","    self.gate = GateUnit(self.gate_in, self.d_conv)\n","\n","  def forward(self, b_long):\n","    b_short = self.short_unit(b_long[:, -self.d_conv:, :, :])\n","    b_gate = torch.cat([b_short, b_long], dim=1)\n","    b_m = self.gate(b_gate)\n","    return b_m"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZVQ3ZRZFLNj","executionInfo":{"status":"ok","timestamp":1618945879550,"user_tz":240,"elapsed":848,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class ReconstBlock(nn.Module):\n","  def __init__(self, d_conv=64, kernel_size=3, stride=1, padding=1):\n","    super(ReconstBlock, self).__init__()\n","    self.d_conv = d_conv\n","    self.recon = nn.Sequential(nn.BatchNorm2d(d_conv), nn.ReLU(),\n","                               nn.Conv2d(in_channels=d_conv, out_channels=1, kernel_size=kernel_size,\n","                                         stride=stride, padding=padding, bias=False))\n","\n","  def forward(self, b_m, x):\n","    y = self.recon(b_m)\n","    return y+x"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEyTxqG0xfAK","executionInfo":{"status":"ok","timestamp":1618945879551,"user_tz":240,"elapsed":841,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class MemNet_SS(nn.Module):\n","    def __init__(self, N_long, N_short, d_conv=64):\n","        super(MemNet_SS, self).__init__()\n","        self.N_long = N_long\n","        self.N_short = N_short\n","        self.d_conv = d_conv\n","        self.fe_block = FeatExtBlock(1, d_conv)\n","        self.mem_blocks = nn.ModuleList([MemBlock(n_mem+1, N_short, d_conv) for n_mem in range(N_long)])\n","        self.recon_block = ReconstBlock(d_conv)\n","        # This part is taken from: https://discuss.pytorch.org/t/contraining-weights-to-sum-to-1/20609/2\n","        # self.eps = 1E-7\n","        # self.linavg = nn.Parameter((1./self.N_long) * torch.ones(N_long), requires_grad=True)\n","\n","    def forward(self, x):\n","      fe = self.fe_block(x)\n","      b_ms = []\n","      b_ms.append(fe)\n","      y = torch.zeros_like(x)\n","      for n_mem in range(self.N_long):\n","        b_long = torch.cat(b_ms, dim=1)\n","        b_ms.append(self.mem_blocks[n_mem](b_long))\n","        # self.linavg = self.linavg.clamp(min=self.eps) # in case weights > 0\n","        # linavg_sum = self.linavg.sum(0, keepdim=True)\n","        # y += self.linavg[n_mem] * self.recon_block(b_ms[-1], x) / linavg_sum\n","      \n","      y = self.recon_block(b_ms[-1], x)\n","      return y"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sszPhXlL1B6b","executionInfo":{"status":"ok","timestamp":1618945879551,"user_tz":240,"elapsed":833,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"d853bd42-69ef-431d-dc95-b9a83dabe0a2"},"source":["def size_check():\n","  p = torch.rand(2, 1, 4, 4)\n","  print(p.size())\n","  d_conv = 8\n","  fe_model = FeatExtBlock(1, d_conv)\n","  q = fe_model(p)\n","  print(q.size())\n","  mem_model = MemBlock(1, 6, d_conv)\n","  r = mem_model(q)\n","  print(r.size())\n","  recon_model = ReconstBlock(d_conv)\n","  s = recon_model(r, p)\n","  print(s.size())\n","  model = MemNet_SS(2, 2, 32)\n","  t = model(p)\n","  print(t.size())\n","\n","size_check()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["torch.Size([2, 1, 4, 4])\n","torch.Size([2, 8, 4, 4])\n","torch.Size([2, 8, 4, 4])\n","torch.Size([2, 1, 4, 4])\n","torch.Size([2, 1, 4, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zkrs29MLquvQ"},"source":["## Dataset Preparation"]},{"cell_type":"code","metadata":{"id":"6R5hB9SzAzpi","executionInfo":{"status":"ok","timestamp":1618945881905,"user_tz":240,"elapsed":763,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class DatasetSR(Dataset):\n","  def __init__(self, images_dir, transform=None):\n","    self.files = []\n","    for dir in images_dir:\n","      self.files += glob.glob(dir + '/*')\n","    self.files = sorted(self.files)\n","    self.transform=transform\n","  \n","  def __len__(self):\n","    return len(self.files)\n","\n","  def __getitem__(self, index):\n","    image_path = self.files[index]\n","    hr_image = PIL.Image.open(image_path)\n","    hr_image = hr_image.convert('YCbCr')\n","    # random_gen = int(image_path.split('/')[-1].split('_')[-2][-1]) + int(image_path.split('/')[-1].split('_')[-1][-5])\n","    # scale = random_gen % 3 + 2 \n","    scale = random.randint(2, 4)\n","    width_down = hr_image.width // scale\n","    heigth_down = hr_image.height // scale\n","    lr_image = hr_image.resize((width_down, heigth_down),\n","                               resample=PIL.Image.BICUBIC)\n","    lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                               resample=PIL.Image.BICUBIC)\n","    \n","    trans_toten = transforms.ToTensor()\n","    hr_ten = trans_toten(hr_image)\n","    lr_ten = trans_toten(lr_image)\n","    hr_ten = rgb_to_ycbcr(hr_ten)\n","    lr_ten = rgb_to_ycbcr(lr_ten)\n","    \n","    if self.transform:\n","            hr = self.transform(hr_image)\n","            lr = self.transform(lr_image)\n","\n","    return hr, lr"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_mQTJeIrHfl"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"2r1QT0atLrnY","executionInfo":{"status":"ok","timestamp":1618945883580,"user_tz":240,"elapsed":526,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["class AttrDict(dict):\n","  def __init__(self, *args, **kwargs):\n","    super(AttrDict, self).__init__(*args, **kwargs)\n","    self.__dict__ = self\n","\n","def weights_init(m):\n","  if isinstance(m, nn.Conv2d):\n","    nn.init.xavier_normal_(m.weight.data, gain=1.0)\n","    # nn.init.zeros_(m.bias.data)\n","\n","# The following function si taken from: https://medium.com/analytics-vidhya/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61\n","def load_ckp(checkpoint_fpath, model):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    model.load_state_dict(checkpoint)\n","    # optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model\n","\n","def train(args):\n","  model = MemNet_SS(args.N_long, args.N_short, args.d_conv)\n","  model.train()\n","  if torch.cuda.is_available():\n","    print(\"sending model to cuda...\")\n","    model = model.cuda()\n","  # optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","  optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.w_decay)\n","  if args['start_from']:\n","    print('starting from {}'.format(args.start_from))\n","    checkpoint = torch.load(args.start_from)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    # model = load_ckp(args['start_from'], model)\n","  else:\n","    print(\"initializing the model...\")\n","    model.apply(weights_init)\n","  \n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.epochs//8, gamma=args.gamma)\n","  criterion = nn.MSELoss(reduction='sum')\n","  trnsfrm = transforms.Compose([transforms.ToTensor()])\n","  # dataset = Dataset(opt.images_dir, opt.patch_size, opt.jpeg_quality, opt.use_fast_loader)\n","  dataset = DatasetSR(images_dir=args.train_dir, transform=trnsfrm)\n","  dataloader = DataLoader(dataset=dataset,\n","                          batch_size=args.batch_size,\n","                          shuffle=True,\n","                          num_workers=args.threads,\n","                          pin_memory=True,\n","                          drop_last=False)\n","  \n","  print('Training starts...')\n","  start = time.time()\n","  train_losses = []\n","  valid_losses = []\n","  for epoch in range(args.epochs):\n","    losses = []\n","    for i, data in enumerate(dataloader):\n","      y, x = data\n","      x = x[:, 0, :, :].unsqueeze(1)\n","      y = y[:, 0, :, :].unsqueeze(1)\n","      if torch.cuda.is_available():\n","        x = x.cuda()\n","        y = y.cuda()\n","      \n","      x_hat = model(x)\n","      # print(\"evaluated\")\n","      loss = (0.5/args.batch_size)*criterion(x_hat, y)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      nn.utils.clip_grad_norm_(model.parameters(), args.clip) \n","      optimizer.step()\n","      losses.append(loss.data.item())\n","\n","    # print(\"moving forward...\")\n","    scheduler.step()\n","    avg_loss = np.mean(losses)\n","    train_losses.append(avg_loss)\n","    time_elapsed = time.time() - start\n","    print('Epoch [%d/%d], Loss: %.4f, Time (s): %d'\n","          % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n","        )\n","    if (epoch+1) % 5 == 0:\n","      patch_checkpoint = os.path.join(args.output_dir, 'memnet_epoch_{}.pt'.format(epoch+1))\n","      torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, patch_checkpoint)\n","      # torch.save(model.state_dict(), os.path.join(args.output_dir, 'memnet_epoch_{}.pt'.format(epoch+1)))\n","\n","  # Plot training curve\n","  plt.figure()\n","  plt.plot(train_losses, \"ro-\", label=\"Train\")\n","  # plt.plot(valid_losses, \"go-\", label=\"Validation\")\n","  plt.legend()\n","  plt.title(\"Loss\")\n","  plt.xlabel(\"Epochs\")\n","  plt.show()\n","  plt.savefig(os.path.join(args.output_dir, \"train_loss.png\"))\n","  \n","  return model"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfRGHdI1PPQu","executionInfo":{"status":"ok","timestamp":1618945908520,"user_tz":240,"elapsed":456,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["drive_dir = '/content/drive/MyDrive/memnet/memnet_ss_4s4m'\n","if not os.path.exists(drive_dir):\n","  os.makedirs(drive_dir)\n","\n","patch_size = 31\n","trainings = ['BSDS200_p'+str(patch_size), 'T91_p'+str(patch_size), 'General100_p'+str(patch_size)]\n","args = AttrDict()\n","args_dict = {\n","    'threads' : 1,\n","    'kernel_size' : 3,\n","    'N_long' : 4,\n","    'N_short' : 4,\n","    'd_conv' : 64,\n","    'lr' : 0.1,\n","    'momentum' : 0.9,\n","    'w_decay' : 1E-4,\n","    'clip' : 0.4,\n","    'gamma' : 0.2,\n","    'batch_size':128,\n","    'patch_size': patch_size,\n","    'epochs' : 80,\n","    'train_dir' : [directory+'/'+trainings[i] for i in range(len(trainings))],\n","    'output_dir' : drive_dir,\n","    'start_from' : ''\n","}\n","args.update(args_dict)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCGLKe5dTbNk","executionInfo":{"status":"ok","timestamp":1618945942108,"user_tz":240,"elapsed":31814,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"26755c9e-6531-48a0-c6eb-2b2533ba83af"},"source":["gdrive_tdir = '/content/drive/MyDrive/memnet'\n","for tri in trainings:\n","  shutil.copy(os.path.join(gdrive_tdir, tri + '.tar.gz'), directory)\n","  file = tarfile.open(os.path.join(directory, tri + '.tar.gz'))\n","  file.extractall('./')\n","  file.close()\n","  print(len(next(os.walk('./'+tri))[2]))\n","\n","# !ls /content/drive/MyDrive/memnet\n","# !cp /content/drive/MyDrive/memnet/T91_p51.tar.gz /content/memnet/T91_p51.tar.gz\n","# with tarfile.open('T91_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !cp /content/drive/MyDrive/memnet/BSDS200_p51.tar.gz /content/memnet/BSDS200_p51.tar.gz\n","# with tarfile.open('BSDS200_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !cp /content/drive/MyDrive/memnet/General100_p51.tar.gz /content/memnet/General100_p51.tar.gz\n","# with tarfile.open('General100_p51.tar.gz') as archive:\n","#   archive.extractall(directory)\n","\n","# !ls /content/memnet/T91_p51/ -1 | wc -l\n","# !ls /content/memnet/BSDS200_p51/ -1 | wc -l\n","# !ls /content/memnet/General100_p31/ -1 | wc -l"],"execution_count":16,"outputs":[{"output_type":"stream","text":["73600\n","13115\n","42803\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uoGJUelbTfGc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1618964855021,"user_tz":240,"elapsed":18912907,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"0a839e0f-4543-4110-91b2-312c14bf1a14"},"source":["memnet = train(args)\n","while True:pass"],"execution_count":17,"outputs":[{"output_type":"stream","text":["sending model to cuda...\n","initializing the model...\n","Training starts...\n","Epoch [1/80], Loss: 4.9763, Time (s): 233\n","Epoch [2/80], Loss: 1.1671, Time (s): 467\n","Epoch [3/80], Loss: 1.1293, Time (s): 701\n","Epoch [4/80], Loss: 1.1108, Time (s): 935\n","Epoch [5/80], Loss: 1.1039, Time (s): 1169\n","Epoch [6/80], Loss: 1.0971, Time (s): 1404\n","Epoch [7/80], Loss: 1.0900, Time (s): 1638\n","Epoch [8/80], Loss: 1.0871, Time (s): 1871\n","Epoch [9/80], Loss: 1.0807, Time (s): 2105\n","Epoch [10/80], Loss: 1.0849, Time (s): 2339\n","Epoch [11/80], Loss: 1.0454, Time (s): 2574\n","Epoch [12/80], Loss: 1.0420, Time (s): 2807\n","Epoch [13/80], Loss: 1.0401, Time (s): 3041\n","Epoch [14/80], Loss: 1.0361, Time (s): 3275\n","Epoch [15/80], Loss: 1.0342, Time (s): 3509\n","Epoch [16/80], Loss: 1.0316, Time (s): 3744\n","Epoch [17/80], Loss: 1.0349, Time (s): 3977\n","Epoch [18/80], Loss: 1.0334, Time (s): 4211\n","Epoch [19/80], Loss: 1.0254, Time (s): 4445\n","Epoch [20/80], Loss: 1.0264, Time (s): 4679\n","Epoch [21/80], Loss: 1.0207, Time (s): 4914\n","Epoch [22/80], Loss: 1.0115, Time (s): 5148\n","Epoch [23/80], Loss: 1.0111, Time (s): 5382\n","Epoch [24/80], Loss: 1.0125, Time (s): 5615\n","Epoch [25/80], Loss: 1.0097, Time (s): 5849\n","Epoch [26/80], Loss: 1.0129, Time (s): 6084\n","Epoch [27/80], Loss: 1.0069, Time (s): 6318\n","Epoch [28/80], Loss: 1.0068, Time (s): 6552\n","Epoch [29/80], Loss: 1.0082, Time (s): 6785\n","Epoch [30/80], Loss: 1.0094, Time (s): 7019\n","Epoch [31/80], Loss: 1.0022, Time (s): 7254\n","Epoch [32/80], Loss: 1.0010, Time (s): 7488\n","Epoch [33/80], Loss: 0.9981, Time (s): 7722\n","Epoch [34/80], Loss: 1.0043, Time (s): 7956\n","Epoch [35/80], Loss: 1.0032, Time (s): 8190\n","Epoch [36/80], Loss: 0.9988, Time (s): 8425\n","Epoch [37/80], Loss: 1.0035, Time (s): 8659\n","Epoch [38/80], Loss: 1.0002, Time (s): 8893\n","Epoch [39/80], Loss: 0.9985, Time (s): 9126\n","Epoch [40/80], Loss: 0.9982, Time (s): 9360\n","Epoch [41/80], Loss: 0.9980, Time (s): 9595\n","Epoch [42/80], Loss: 0.9990, Time (s): 9829\n","Epoch [43/80], Loss: 1.0055, Time (s): 10063\n","Epoch [44/80], Loss: 1.0015, Time (s): 10296\n","Epoch [45/80], Loss: 0.9972, Time (s): 10530\n","Epoch [46/80], Loss: 1.0004, Time (s): 10765\n","Epoch [47/80], Loss: 1.0009, Time (s): 10999\n","Epoch [48/80], Loss: 0.9981, Time (s): 11233\n","Epoch [49/80], Loss: 1.0016, Time (s): 11466\n","Epoch [50/80], Loss: 0.9970, Time (s): 11700\n","Epoch [51/80], Loss: 0.9987, Time (s): 11935\n","Epoch [52/80], Loss: 0.9976, Time (s): 12169\n","Epoch [53/80], Loss: 0.9978, Time (s): 12403\n","Epoch [54/80], Loss: 0.9969, Time (s): 12636\n","Epoch [55/80], Loss: 0.9958, Time (s): 12870\n","Epoch [56/80], Loss: 0.9978, Time (s): 13105\n","Epoch [57/80], Loss: 1.0006, Time (s): 13339\n","Epoch [58/80], Loss: 0.9992, Time (s): 13572\n","Epoch [59/80], Loss: 0.9995, Time (s): 13806\n","Epoch [60/80], Loss: 0.9960, Time (s): 14040\n","Epoch [61/80], Loss: 0.9962, Time (s): 14276\n","Epoch [62/80], Loss: 0.9980, Time (s): 14509\n","Epoch [63/80], Loss: 0.9951, Time (s): 14743\n","Epoch [64/80], Loss: 0.9996, Time (s): 14977\n","Epoch [65/80], Loss: 1.0026, Time (s): 15211\n","Epoch [66/80], Loss: 0.9977, Time (s): 15447\n","Epoch [67/80], Loss: 0.9936, Time (s): 15681\n","Epoch [68/80], Loss: 0.9983, Time (s): 15916\n","Epoch [69/80], Loss: 0.9990, Time (s): 16150\n","Epoch [70/80], Loss: 0.9992, Time (s): 16385\n","Epoch [71/80], Loss: 1.0013, Time (s): 16620\n","Epoch [72/80], Loss: 0.9940, Time (s): 16854\n","Epoch [73/80], Loss: 0.9974, Time (s): 17088\n","Epoch [74/80], Loss: 0.9989, Time (s): 17321\n","Epoch [75/80], Loss: 1.0010, Time (s): 17555\n","Epoch [76/80], Loss: 0.9991, Time (s): 17790\n","Epoch [77/80], Loss: 0.9994, Time (s): 18024\n","Epoch [78/80], Loss: 0.9982, Time (s): 18258\n","Epoch [79/80], Loss: 1.0036, Time (s): 18491\n","Epoch [80/80], Loss: 1.0010, Time (s): 18725\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b289ffe118e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmemnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"MihpSN0ndWZj","executionInfo":{"status":"ok","timestamp":1618964975452,"user_tz":240,"elapsed":454,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}}},"source":["drive_dir = '/content/drive/MyDrive/memnet/memnet_ss_4s4m'\n","if not os.path.exists(drive_dir):\n","  os.makedirs(drive_dir)\n","\n","patch_size = 31\n","trainings = ['BSDS200_p'+str(patch_size), 'T91_p'+str(patch_size), 'General100_p'+str(patch_size)]\n","args = AttrDict()\n","args_dict = {\n","    'threads' : 1,\n","    'kernel_size' : 3,\n","    'N_long' : 4,\n","    'N_short' : 4,\n","    'd_conv' : 64,\n","    'lr' : 0.1,\n","    'momentum' : 0.9,\n","    'w_decay' : 1E-4,\n","    'clip' : 0.4,\n","    'gamma' : 0.2,\n","    'batch_size':128,\n","    'patch_size': patch_size,\n","    'epochs' : 80,\n","    'train_dir' : [directory+'/'+trainings[i] for i in range(len(trainings))],\n","    'output_dir' : drive_dir,\n","    'start_from' : ''\n","}\n","args.update(args_dict)\n","\n","patch_checkpoint = os.path.join(args.output_dir, 'memnet_epoch_{}.pt'.format(80))\n","torch.save({\n","      'epoch': 80,\n","      'model_state_dict': memnet.state_dict(),\n","      'optimizer_state_dict': None,\n","      'loss': None,\n","      }, patch_checkpoint)"],"execution_count":18,"outputs":[]}]}