{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"memnet_ms_vartech.ipynb","provenance":[],"collapsed_sections":["KuMYUztgp6h_","uO1JLjItq4Pd"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNURuftZuLmUv1ehfiThi+H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DS9_H89C0o1b"},"source":["## Module setup"]},{"cell_type":"code","metadata":{"id":"khmWyAd17fbU"},"source":["import os\n","import tarfile\n","import glob\n","import io\n","import random\n","from tqdm import tqdm\n","import PIL\n","from IPython.display import display, Image\n","\n","import numpy as np\n","from six.moves.urllib.request import urlretrieve\n","import matplotlib.pyplot as plt\n","\n","\n","import torch\n","from torch import nn\n","from torch.nn import Parameter\n","# import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","# from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision import transforms\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KuMYUztgp6h_"},"source":["## Helper \n","# RGB -> YCbCr and YCbCr -> RGB image conversion\n","This part is taken from Kornia\n","https://kornia.readthedocs.io/en/latest/_modules/kornia/color/ycbcr.html"]},{"cell_type":"code","metadata":{"id":"ZJlq-WlfqQ69"},"source":["def rgb_to_ycbcr(image: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Convert an RGB image to YCbCr.\n","\n","    Args:\n","        image (torch.Tensor): RGB Image to be converted to YCbCr with shape :math:`(*, 3, H, W)`.\n","\n","    Returns:\n","        torch.Tensor: YCbCr version of the image with shape :math:`(*, 3, H, W)`.\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> output = rgb_to_ycbcr(input)  # 2x3x4x5\n","    \"\"\"\n","    if not isinstance(image, torch.Tensor):\n","        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n","            type(image)))\n","\n","    if len(image.shape) < 3 or image.shape[-3] != 3:\n","        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n","                         .format(image.shape))\n","\n","    r: torch.Tensor = image[..., 0, :, :]\n","    g: torch.Tensor = image[..., 1, :, :]\n","    b: torch.Tensor = image[..., 2, :, :]\n","\n","    delta: float = 0.5\n","    y: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n","    cb: torch.Tensor = (b - y) * 0.564 + delta\n","    cr: torch.Tensor = (r - y) * 0.713 + delta\n","    return torch.stack([y, cb, cr], -3)\n","\n","\n","\n","def ycbcr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Convert an YCbCr image to RGB.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Args:\n","        image (torch.Tensor): YCbCr Image to be converted to RGB with shape :math:`(*, 3, H, W)`.\n","\n","    Returns:\n","        torch.Tensor: RGB version of the image with shape :math:`(*, 3, H, W)`.\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> output = ycbcr_to_rgb(input)  # 2x3x4x5\n","    \"\"\"\n","    if not isinstance(image, torch.Tensor):\n","        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n","            type(image)))\n","\n","    if len(image.shape) < 3 or image.shape[-3] != 3:\n","        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n","                         .format(image.shape))\n","\n","    y: torch.Tensor = image[..., 0, :, :]\n","    cb: torch.Tensor = image[..., 1, :, :]\n","    cr: torch.Tensor = image[..., 2, :, :]\n","\n","    delta: float = 0.5\n","    cb_shifted: torch.Tensor = cb - delta\n","    cr_shifted: torch.Tensor = cr - delta\n","\n","    r: torch.Tensor = y + 1.403 * cr_shifted\n","    g: torch.Tensor = y - 0.714 * cr_shifted - 0.344 * cb_shifted\n","    b: torch.Tensor = y + 1.773 * cb_shifted\n","    return torch.stack([r, g, b], -3)\n","\n","\n","\n","class RgbToYcbcr(nn.Module):\n","    \"\"\"Convert an image from RGB to YCbCr.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Returns:\n","        torch.Tensor: YCbCr version of the image.\n","\n","    Shape:\n","        - image: :math:`(*, 3, H, W)`\n","        - output: :math:`(*, 3, H, W)`\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> ycbcr = RgbToYcbcr()\n","        >>> output = ycbcr(input)  # 2x3x4x5\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(RgbToYcbcr, self).__init__()\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        return rgb_to_ycbcr(image)\n","\n","\n","\n","class YcbcrToRgb(nn.Module):\n","    \"\"\"Convert an image from YCbCr to Rgb.\n","\n","    The image data is assumed to be in the range of (0, 1).\n","\n","    Returns:\n","        torch.Tensor: RGB version of the image.\n","\n","    Shape:\n","        - image: :math:`(*, 3, H, W)`\n","        - output: :math:`(*, 3, H, W)`\n","\n","    Examples:\n","        >>> input = torch.rand(2, 3, 4, 5)\n","        >>> rgb = YcbcrToRgb()\n","        >>> output = rgb(input)  # 2x3x4x5\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(YcbcrToRgb, self).__init__()\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        return ycbcr_to_rgb(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eO181ezuqXK6"},"source":["## Directory setup and preparing the datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6cRBsi_0Z2s","executionInfo":{"status":"ok","timestamp":1618661484726,"user_tz":240,"elapsed":23041,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"ccd6e1dc-44cb-490e-eaba-d79a8516bc50"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z9bnrCCa4Y_E"},"source":["drive_dir = '/content/drive/MyDrive/memnet/memnet_ms_vartech'\n","if not os.path.exists(drive_dir):\n","  os.makedirs(drive_dir)\n","\n","directory = '/content/memnet'\n","if not os.path.exists(directory):\n","  os.makedirs(directory)\n","\n","os.chdir(directory)\n","# %mkdir -p /content/drive/MyDrive/memnet\n","# %cd /content/drive/MyDrive/memnet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcghdQBZzYhe","executionInfo":{"status":"ok","timestamp":1618661530306,"user_tz":240,"elapsed":26828,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"4eff80cc-2500-4f99-8e99-1512e3f1260a"},"source":["!ls /content/drive/MyDrive/memnet\n","!cp /content/drive/MyDrive/memnet/t91_crop_training.tar.gz /content/memnet/t91_crop_training.tar.gz\n","with tarfile.open('t91_crop_training.tar.gz') as archive:\n","  archive.extractall(directory)\n","\n","!cp /content/drive/MyDrive/memnet/bsds_crop_training.tar.gz /content/memnet/bsds_crop_training.tar.gz\n","with tarfile.open('bsds_crop_training.tar.gz') as archive:\n","  archive.extractall(directory)\n","\n","\n","!ls /content/memnet/t91_crop_training/ -1 | wc -l\n","!ls /content/memnet/bsds_crop_training/ -1 | wc -l"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BSDS300-images.tgz\t   memnet_ms_t91      sample_tests\n","bsds_crop_training.tar.gz  memnet_ms_vartech  t91_crop_training.tar.gz\n","memnet_ms_bsds\t\t   memnet_ss_bsds\n","13115\n","73600\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oAC9_hm-gqUr"},"source":["# bsds = 'https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz'\n","# bsds_file = directory+'/bsds300'\n","# urlretrieve(bsds, bsds_file)\n","# with tarfile.open(bsds_file) as archive:\n","#   archive.extractall(directory)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uO1JLjItq4Pd"},"source":["## Model : Multi-Supervised MemNet "]},{"cell_type":"code","metadata":{"id":"A5rUr_6uDYZg"},"source":["class FeatExtBlock(nn.Module):\n","    def __init__(self, in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1):\n","        super(FeatExtBlock, self).__init__()\n","        self.feature = nn.Sequential(nn.BatchNorm2d(in_channels), nn.ReLU(),\n","                                     nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n","                                               kernel_size=kernel_size, stride=stride,\n","                                               padding=padding, bias=False))\n","\n","    def forward(self, x):\n","        fe_out = self.feature(x)\n","        return fe_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoywv9Bk5HNp"},"source":["class ResidualBlock(nn.Module):\n","  def __init__(self, n_layers=2, n_channels=32, kernel_size=3, stride=1, padding=1):\n","    super(ResidualBlock, self).__init__()\n","    layers = []\n","    for _ in range(n_layers):\n","      layers.append(nn.BatchNorm2d(n_channels))\n","      layers.append(nn.ReLU())\n","      layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n","                              stride=stride, padding=padding, bias=False))\n","    \n","    self.residual = nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    out = self.residual(x)\n","    return out + x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awmHxvKb69Xl"},"source":["class RecursiveUnit(nn.Module):\n","  def __init__(self, n_short=6, d_conv=32):\n","    super(RecursiveUnit, self).__init__()\n","    self.n_short = n_short\n","    self.residual = nn.ModuleList([ResidualBlock(n_layers=2, n_channels=d_conv) for _ in range(n_short)])\n","\n","  def forward(self, b_prev_m):\n","    Hs = []\n","    Hs.append(self.residual[0](b_prev_m))\n","    for rec in range(1, self.n_short):\n","      Hs.append(self.residual[rec](Hs[-1]))\n","    b_short = torch.cat(Hs, dim=1)\n","    return b_short"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CqUd6dgpRBgA"},"source":["class GateUnit(nn.Module):\n","    def __init__(self, in_channels, d_conv=32):\n","        super(GateUnit, self).__init__()\n","        self.in_channels = in_channels\n","\n","        \n","        self.gate_layer = nn.Sequential(nn.BatchNorm2d(in_channels), nn.ReLU(),\n","                                  nn.Conv2d(in_channels=self.in_channels, out_channels=d_conv, kernel_size=1,\n","                                            stride=1, padding=0, bias=False))\n","\n","    def forward(self, b_gate):\n","        b_mem = self.gate_layer(b_gate)\n","        return b_mem"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8MftIwz-CIk"},"source":["class MemBlock(nn.Module):\n","  def __init__(self, n_long, N_short=6, d_conv=32):\n","    super(MemBlock, self).__init__()\n","    self.n_long = n_long\n","    self.N_long = N_short\n","    self.d_conv = d_conv\n","    self.gate_in = (n_long + N_short) * d_conv\n","    self.short_unit = RecursiveUnit(N_short, d_conv)\n","    self.gate = GateUnit(self.gate_in, self.d_conv)\n","\n","  def forward(self, b_long):\n","    b_short = self.short_unit(b_long[:, -self.d_conv:, :, :])\n","    b_gate = torch.cat([b_short, b_long], dim=1)\n","    b_m = self.gate(b_gate)\n","    return b_m"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZVQ3ZRZFLNj"},"source":["class ReconstBlock(nn.Module):\n","  def __init__(self, d_conv=32, kernel_size=3, stride=1, padding=1):\n","    super(ReconstBlock, self).__init__()\n","    self.d_conv = d_conv\n","    self.recon = nn.Sequential(nn.BatchNorm2d(d_conv),\n","                               nn.ReLU(),\n","                               nn.Conv2d(in_channels=d_conv, out_channels=1, kernel_size=kernel_size,\n","                                         stride=stride, padding=padding, bias=False))\n","\n","  def forward(self, b_m, x):\n","    y = self.recon(b_m)\n","    return y+x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEyTxqG0xfAK"},"source":["class MemNet(nn.Module):\n","    def __init__(self, N_long, N_short, d_conv=32):\n","        super(MemNet, self).__init__()\n","        self.N_long = N_long\n","        self.N_short = N_short\n","        self.d_conv = d_conv\n","        self.fe_block = FeatExtBlock(1, d_conv)\n","        self.mem_blocks = nn.ModuleList([MemBlock(n_mem+1, N_short, d_conv) for n_mem in range(N_long)])\n","        self.recon_block = ReconstBlock(d_conv)\n","        # This part is taken from: https://discuss.pytorch.org/t/contraining-weights-to-sum-to-1/20609/2\n","        self.eps = 1E-7\n","        self.linavg = nn.Parameter((1./self.N_long) * torch.ones(N_long), requires_grad=True)\n","\n","    def forward(self, x):\n","      fe = self.fe_block(x)\n","      b_ms = []\n","      b_ms.append(fe)\n","      y = torch.zeros_like(x)\n","      for n_mem in range(self.N_long):\n","        b_long = torch.cat(b_ms, dim=1)\n","        b_ms.append(self.mem_blocks[n_mem](b_long))\n","        # self.linavg = self.linavg.clamp(min=self.eps) # in case weights > 0\n","        linavg_sum = self.linavg.sum(0, keepdim=True)\n","        y += self.linavg[n_mem] * self.recon_block(b_ms[-1], x) / linavg_sum\n","\n","      return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sszPhXlL1B6b","executionInfo":{"status":"ok","timestamp":1618661533851,"user_tz":240,"elapsed":667,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"05d6c115-0165-4513-e17d-c4b4d7cd6770"},"source":["def size_check():\n","  p = torch.rand(2, 1, 4, 4)\n","  print(p.size())\n","  d_conv = 8\n","  fe_model = FeatExtBlock(1, d_conv)\n","  q = fe_model(p)\n","  print(q.size())\n","  mem_model = MemBlock(1, 6, d_conv)\n","  r = mem_model(q)\n","  print(r.size())\n","  recon_model = ReconstBlock(d_conv)\n","  s = recon_model(r, p)\n","  print(s.size())\n","  model = MemNet(2, 2, 32)\n","  t = model(p)\n","  print(t.size())\n","\n","size_check()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([2, 1, 4, 4])\n","torch.Size([2, 8, 4, 4])\n","torch.Size([2, 8, 4, 4])\n","torch.Size([2, 1, 4, 4])\n","torch.Size([2, 1, 4, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zkrs29MLquvQ"},"source":["## Dataset Preparation"]},{"cell_type":"code","metadata":{"id":"6R5hB9SzAzpi"},"source":["class DatasetSR(Dataset):\n","  def __init__(self, images_dir, scale=None, patch_size=None, stride=21, transform=None):\n","    # self.dir = images_dir\n","    self.files = []\n","    for dir in images_dir:\n","      self.files += glob.glob(dir + '/*')\n","    self.files = sorted(self.files)\n","    self.scale = scale\n","    self.patch = patch_size\n","    self.stride = stride\n","    self.transform=transform\n","  \n","  def __len__(self):\n","    return len(self.files)\n","\n","  def random_lr_gen(self, hr_image, scale):\n","    chosen_tech = random.randint(0, 5)\n","    width_down = hr_image.width // scale\n","    heigth_down = hr_image.height // scale\n","    if chosen_tech==0:\n","      lr_image = hr_image.resize((width_down, heigth_down),\n","                                resample=PIL.Image.LANCZOS)\n","      lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                                resample=PIL.Image.LANCZOS)\n","    elif chosen_tech==1:\n","      lr_image = hr_image.resize((width_down, heigth_down),\n","                                resample=PIL.Image.NEAREST)\n","      lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                                resample=PIL.Image.NEAREST)\n","    elif chosen_tech==2:\n","      lr_image = hr_image.resize((width_down, heigth_down),\n","                                resample=PIL.Image.BICUBIC)\n","      lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                                resample=PIL.Image.BICUBIC)\n","    elif chosen_tech==3:\n","      lr_image = hr_image.resize((width_down, heigth_down),\n","                                resample=PIL.Image.LINEAR)\n","      lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                                resample=PIL.Image.LINEAR)\n","    elif chosen_tech==4:\n","      lr_image = hr_image.resize((width_down, heigth_down),\n","                                resample=PIL.Image.BOX)\n","      lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                                resample=PIL.Image.BOX)\n","    elif chosen_tech==5:\n","      lr_image = hr_image.resize((width_down, heigth_down),\n","                                resample=PIL.Image.HAMMING)\n","      lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                                resample=PIL.Image.HAMMING)\n","    \n","    return lr_image\n","\n","  def __getitem__(self, index):\n","    image_path = self.files[index]\n","    # image_path = os.path.join(self.dir, self.files[index])\n","\n","    hr_image = PIL.Image.open(image_path)\n","    hr_image = hr_image.convert('YCbCr')\n","    \n","    # random_gen = int(image_path.split('/')[-1].split('_')[-2][-1]) + int(image_path.split('/')[-1].split('_')[-1][-5])\n","    # scale = random_gen % 3 + 2 \n","    scale = random.randint(2, 4)\n","    # width_down = hr_image.width // scale\n","    # heigth_down = hr_image.height // scale\n","    # lr_image = hr_image.resize((width_down, heigth_down),\n","    #                            resample=PIL.Image.BICUBIC)\n","    # lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","    #                            resample=PIL.Image.BICUBIC)\n","    lr_image = self.random_lr_gen(hr_image, scale)\n","    \n","    trans_toten = transforms.ToTensor()\n","    hr_ten = trans_toten(hr_image)\n","    lr_ten = trans_toten(lr_image)\n","    hr_ten = rgb_to_ycbcr(hr_ten)\n","    lr_ten = rgb_to_ycbcr(lr_ten)\n","    \n","    if self.transform:\n","            hr = self.transform(hr_image)\n","            lr = self.transform(lr_image)\n","\n","    return hr, lr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_mQTJeIrHfl"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"2r1QT0atLrnY"},"source":["class AttrDict(dict):\n","  def __init__(self, *args, **kwargs):\n","    super(AttrDict, self).__init__(*args, **kwargs)\n","    self.__dict__ = self\n","\n","def weights_init(m):\n","  if isinstance(m, nn.Conv2d):\n","    nn.init.xavier_normal_(m.weight.data, gain=1.0)\n","    # nn.init.zeros_(m.bias.data)\n","\n","# The following function si taken from: https://medium.com/analytics-vidhya/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61\n","def load_ckp(checkpoint_fpath, model):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    model.load_state_dict(checkpoint)\n","    # optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model\n","\n","def train(args):\n","  model = MemNet(args.N_long, args.N_short, args.d_conv)\n","  model.train()\n","  if torch.cuda.is_available():\n","    model = model.cuda()\n","  \n","  # optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","  optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.w_decay)\n","  if args['start_from']:\n","    print('starting from ')\n","    model = load_ckp(args['start_from'], model)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.epochs//4, gamma=0.1)\n","  criterion = nn.MSELoss()\n","  # model.apply(weights_init)\n","  trnsfrm = transforms.Compose([transforms.ToTensor()])\n","  # dataset = Dataset(opt.images_dir, opt.patch_size, opt.jpeg_quality, opt.use_fast_loader)\n","  dataset = DatasetSR(images_dir=args.train_dir, scale=args.scale, patch_size=args.patch_size, transform=trnsfrm)\n","  dataloader = DataLoader(dataset=dataset,\n","                          batch_size=args.batch_size,\n","                          shuffle=True,\n","                          num_workers=args.threads,\n","                          pin_memory=True,\n","                          drop_last=False)\n","  \n","  print('Training starts...')\n","  start = time.time()\n","  train_losses = []\n","  valid_losses = []\n","  for epoch in range(14, args.epochs):\n","    losses = []\n","    for i, data in enumerate(dataloader):\n","      y, x = data\n","      x = x[:, 0, :, :].unsqueeze(1)\n","      y = y[:, 0, :, :].unsqueeze(1)\n","      if torch.cuda.is_available():\n","        x = x.cuda()\n","        y = y.cuda()\n","      \n","      x_hat = model(x)\n","\n","      loss = 0.5*args.patch_size*criterion(x_hat, y)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      nn.utils.clip_grad_norm_(model.parameters(), args.clip) \n","      optimizer.step()\n","      losses.append(loss.data.item())\n","\n","    scheduler.step()\n","    avg_loss = np.mean(losses)\n","    train_losses.append(avg_loss)\n","    time_elapsed = time.time() - start\n","    print('Epoch [%d/%d], Loss: %.4f, Time (s): %d'\n","          % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n","        )\n","    if epoch % 1 == 0:\n","      torch.save(model.state_dict(), os.path.join(args.output_dir, 'memnet_epoch_{}.pth'.format(epoch)))\n","\n","  # Plot training curve\n","  plt.figure()\n","  plt.plot(train_losses, \"ro-\", label=\"Train\")\n","  # plt.plot(valid_losses, \"go-\", label=\"Validation\")\n","  plt.legend()\n","  plt.title(\"Loss\")\n","  plt.xlabel(\"Epochs\")\n","  plt.show()\n","  plt.savefig(args.output_dir + \"train_loss.png\")\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfRGHdI1PPQu","executionInfo":{"status":"ok","timestamp":1618670842244,"user_tz":240,"elapsed":3614612,"user":{"displayName":"Farhad Javid","photoUrl":"","userId":"01136343679243056491"}},"outputId":"09ff85be-85a3-40cb-947d-10a289a6b20f"},"source":["args = AttrDict()\n","args_dict = {\n","    'threads' : 1,\n","    'kernel_size' : 3,\n","    'N_long' : 6,\n","    'N_short' : 6,\n","    'd_conv' : 64,\n","    'lr' : 0.05,\n","    'momentum' : 0.9,\n","    'w_decay' : 1E-4,\n","    'clip' : 0.4,\n","    'batch_size': 64,\n","    'patch_size': 31,\n","    'scale' : 2,\n","    'epochs' : 40,\n","    'train_dir' : [directory + '/bsds_crop_training/', directory + '/t91_crop_training/'],\n","    'output_dir' : drive_dir,\n","    'start_from' : '/content/drive/MyDrive/memnet/memnet_ms_vartech/memnet_epoch_13.pth' # default is ''\n","}\n","args.update(args_dict)\n","memnet = train(args)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["starting from \n","Training starts...\n","Epoch [15/40], Loss: 0.0449, Time (s): 355\n","Epoch [16/40], Loss: 0.0446, Time (s): 711\n","Epoch [17/40], Loss: 0.0446, Time (s): 1066\n","Epoch [18/40], Loss: 0.0449, Time (s): 1421\n","Epoch [19/40], Loss: 0.0448, Time (s): 1776\n","Epoch [20/40], Loss: 0.0447, Time (s): 2130\n","Epoch [21/40], Loss: 0.0444, Time (s): 2485\n","Epoch [22/40], Loss: 0.0444, Time (s): 2840\n","Epoch [23/40], Loss: 0.0444, Time (s): 3195\n","Epoch [24/40], Loss: 0.0446, Time (s): 3550\n","Epoch [25/40], Loss: 0.0434, Time (s): 3905\n","Epoch [26/40], Loss: 0.0429, Time (s): 4260\n","Epoch [27/40], Loss: 0.0430, Time (s): 4615\n","Epoch [28/40], Loss: 0.0428, Time (s): 4970\n","Epoch [29/40], Loss: 0.0429, Time (s): 5325\n","Epoch [30/40], Loss: 0.0429, Time (s): 5680\n","Epoch [31/40], Loss: 0.0427, Time (s): 6034\n","Epoch [32/40], Loss: 0.0427, Time (s): 6389\n","Epoch [33/40], Loss: 0.0428, Time (s): 6744\n","Epoch [34/40], Loss: 0.0428, Time (s): 7100\n","Epoch [35/40], Loss: 0.0423, Time (s): 7455\n","Epoch [36/40], Loss: 0.0425, Time (s): 7810\n","Epoch [37/40], Loss: 0.0425, Time (s): 8165\n","Epoch [38/40], Loss: 0.0423, Time (s): 8520\n","Epoch [39/40], Loss: 0.0423, Time (s): 8875\n","Epoch [40/40], Loss: 0.0423, Time (s): 9230\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"uoGJUelbTfGc"},"source":["while True:pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmPzA7QDrR_k"},"source":["## Testing"]},{"cell_type":"code","metadata":{"id":"4hD6dLXUlJdT"},"source":["# args = AttrDict()\n","# args_dict = {\n","#     'threads' : 1,\n","#     'kernel_size' : 3,\n","#     'N_long' : 6,\n","#     'N_short' : 6,\n","#     'd_conv' : 64,\n","#     'lr' : 0.1,\n","#     'batch_size': 32,\n","#     'patch_size': 41,\n","#     'scale' : 2,\n","#     'epochs' : 20,\n","#     'train_dir' : directory + '/BSDS300/images/train/',\n","#     'output_dir' : '/content/drive/MyDrive/memnet/'\n","# }\n","model = MemNet(args_dict['N_long'], args_dict['N_short'], args_dict['d_conv'])\n","model.load_state_dict(torch.load(os.path.join(args_dict['output_dir'], 'memnet_epoch_{}.pth'.format(3)),\n","                                 map_location=torch.device('cpu')))\n","\n","model.eval()\n","model.requires_grad_ = False\n","for param in model.parameters():\n","    param.require_grads = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWufvhHXyOTF"},"source":["crop_size = 200\n","down_sample = 3\n","files = sorted(glob.glob(directory + '/BSDS300/images/train/' + '/*'))\n","index = random.randint(0,199)\n","hr_image = PIL.Image.open(files[index])\n","crop_bl = [100, 100]#[random.randint(0, hr_image.width - crop_size),\n","           #random.randint(0, hr_image.height - crop_size)]\n","hr_image = hr_image.crop((crop_bl[0], crop_bl[1],\n","                          crop_bl[0]+crop_size, crop_bl[1]+crop_size))\n","lr_width = hr_image.width // down_sample\n","lr_height = hr_image.height // down_sample\n","lr_image = hr_image.resize((lr_width, lr_height),\n","                              resample=PIL.Image.BICUBIC, box=None, reducing_gap=None)\n","lr_image = lr_image.resize((hr_image.width, hr_image.height),\n","                              resample=PIL.Image.BICUBIC, box=None, reducing_gap=None)\n","\n","trsfm = transforms.Compose([transforms.ToTensor()])\n","lr_ten = trsfm(lr_image)\n","lr_ten = rgb_to_ycbcr(lr_ten)\n","\n","pred_c = model(lr_ten.unsqueeze(0)[:, 0, :, :].unsqueeze(1))\n","# diff = pred_c - lr_ten.unsqueeze(0)[:, 0, :, :].unsqueeze(1)\n","pred_c = pred_c.detach()\n","pred_ten = lr_ten.detach().clone()\n","pred_ten[0, :, :] = pred_c[0, 0, :, :]\n","pred_ten = pred_ten.detach()\n","\n","pred_arr = ((255.0*ycbcr_to_rgb(pred_ten)).numpy().astype(np.uint8())).transpose([1, 2, 0])\n","pred_image = PIL.Image.fromarray(pred_arr, 'RGB')\n","display(hr_image)\n","display(lr_image)\n","display(pred_image)\n","\n","hr_ten = trsfm(hr_image)\n","hr_ten = rgb_to_ycbcr(hr_ten)\n","hr_bl_arr = ((255.0*hr_ten).numpy().astype(np.uint8())).transpose([1, 2, 0])\n","lr_bl_arr = ((255.0*lr_ten).numpy().astype(np.uint8())).transpose([1, 2, 0])\n","hr_bl_image = PIL.Image.fromarray(hr_bl_arr[:, :, 0], 'L')\n","lr_bl_image = PIL.Image.fromarray(lr_bl_arr[:, :, 0], 'L')\n","display(hr_bl_image)\n","display(lr_bl_image)\n","\n","pred_bl_arr = (255.0*pred_c.squeeze()).numpy().astype(np.uint8())\n","pred_bl_image = PIL.Image.fromarray(pred_bl_arr, 'L')\n","display(pred_bl_image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1822dtu3CV4"},"source":["hr_image = PIL.Image.open('/content/drive/MyDrive/memnet/sample_tests/IMG-4035.JPG')\n","en_width = int(1.5 * hr_image.width)\n","en_height = int(1.5 * hr_image.height)\n","en_image = hr_image.resize((en_width, en_height),\n","                              resample=PIL.Image.BICUBIC, box=None, reducing_gap=None)\n","\n","trsfm = transforms.Compose([transforms.ToTensor()])\n","en_ten = trsfm(en_image)\n","en_ten = rgb_to_ycbcr(en_ten)\n","\n","pred_c = model(en_ten.unsqueeze(0)[:, 0, :, :].unsqueeze(1))\n","pred_c = pred_c.detach()\n","pred_ten = en_ten.detach().clone()\n","pred_ten[0, :, :] = pred_c[0, 0, :, :]\n","pred_ten = pred_ten.detach()\n","\n","pred_arr_cv = cv2.normalize((ycbcr_to_rgb(pred_ten).squeeze()).numpy().transpose([1, 2, 0]),\n","                            None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n","pred_arr = ((255.0*ycbcr_to_rgb(pred_ten)).numpy().astype(np.uint8())).transpose([1, 2, 0])\n","pred_image = PIL.Image.fromarray(pred_arr, 'RGB')\n","pred_image_cv = PIL.Image.fromarray(pred_arr_cv, 'RGB')\n","display(hr_image.transpose(PIL.Image.ROTATE_270))\n","display(en_image.transpose(PIL.Image.ROTATE_270))\n","display(pred_image.transpose(PIL.Image.ROTATE_270))\n","display(pred_image_cv.transpose(PIL.Image.ROTATE_270))"],"execution_count":null,"outputs":[]}]}